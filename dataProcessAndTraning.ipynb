{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data\\database.sqlite\n",
      "./data\\dataNotes.md\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('./data'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Any results you write to the current directory are saved as output.\n",
    "pd.set_option('display.max_rows', 40)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now move on to the first round of data processing, where values which the full time result does not exist are removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial dataframe shape: (179807, 172)\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import sklearn, numpy\n",
    "import category_encoders as ce\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "con = sqlite3.connect('./data/database.sqlite')\n",
    "\n",
    "df = pd.read_sql_query(\"select * from football_data;\", con)\n",
    "#df = df.sample(frac=0.01)\n",
    "\n",
    "print(\"Initial dataframe shape:\", df.shape)\n",
    "\n",
    "index_names = df[(df['FTR'] != 'H') & (df['FTR'] != 'A') & (df['FTR'] != 'D')].index\n",
    "\n",
    "#df = df.sort_values('Datetime')\n",
    "\n",
    "df.drop(index_names, inplace=True)\n",
    "\n",
    "column_names = list(df.columns.values)\n",
    "#df.dropna(inplace=True, thresh=int(0.8*df.shape[0]) , axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition of data\n",
    "\n",
    "### Match Information\n",
    "- Div = League Division\n",
    "- Date = Match Date (dd/mm/yy)\n",
    "- Time = Time of match kick off\n",
    "- HomeTeam = Home Team\n",
    "- AwayTeam = Away Team\n",
    "- FTHG and HG = Full Time Home Team Goals\n",
    "- FTAG and AG = Full Time Away Team Goals\n",
    "- FTR and Res = Full Time Result (H=Home Win, D=Draw, A=Away Win)\n",
    "- HTHG = Half Time Home Team Goals\n",
    "- HTAG = Half Time Away Team Goals\n",
    "- HTR = Half Time Result (H=Home Win, D=Draw, A=Away Win)\n",
    "\n",
    "### Match Statistics (where available)\n",
    "- Attendance = Crowd Attendance\n",
    "- Referee = Match Referee\n",
    "- HS = Home Team Shots\n",
    "- AS = Away Team Shots\n",
    "- HST = Home Team Shots on Target\n",
    "- AST = Away Team Shots on Target\n",
    "- HHW = Home Team Hit Woodwork\n",
    "- AHW = Away Team Hit Woodwork\n",
    "- HC = Home Team Corners\n",
    "- AC = Away Team Corners\n",
    "- HF = Home Team Fouls Committed\n",
    "- AF = Away Team Fouls Committed\n",
    "- HFKC = Home Team Free Kicks Conceded\n",
    "- AFKC = Away Team Free Kicks Conceded\n",
    "- HO = Home Team Offsides\n",
    "- AO = Away Team Offsides\n",
    "- HY = Home Team Yellow Cards\n",
    "- AY = Away Team Yellow Cards\n",
    "- HR = Home Team Red Cards\n",
    "- AR = Away Team Red Cards\n",
    "- HBP = Home Team Bookings Points (10 = yellow, 25 = red)\n",
    "- ABP = Away Team Bookings Points (10 = yellow, 25 = red)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(179803, 140)\n"
     ]
    }
   ],
   "source": [
    "# These attributes contain significant number of null values\n",
    "\n",
    "noneSet = []\n",
    "noneSet = [\n",
    "    'ABP', 'AFKC', 'AHW', 'AO', 'AT', 'Attendance', 'BSA', 'BSD', 'BSH',\n",
    "    'Bb1X2', 'BbAH', 'BbAHh', 'BbAv<2.5', 'BbAv>2.5', 'BbAvA', 'BbAvAHA',\n",
    "    'BbAvAHH', 'BbAvD', 'BbAvH', 'BbMx<2.5', 'BbMx>2.5', 'B365AH', 'BbMxA',\n",
    "    'BbMxAHA', 'BbMxAHH', 'BbMxD', 'BbMxH', 'BbOU', 'GB<2.5', 'GB>2.5', 'GBA',\n",
    "    'GBAH', 'GBAHA', 'GBAHH', 'GBD', 'GBH', 'HBP', 'HFKC', 'HHW', 'HO', 'HT',\n",
    "    'LBAH', 'LBAHA', 'LBAHH', 'SBA', 'SBD', 'SBH', 'SJA', 'SJD', 'SJH', 'SOA',\n",
    "    'SOD', 'SOH', 'SYA', 'SYD', 'SYH', 'PA', 'PD', 'PH', 'LBH', 'LBD', 'LBA'\n",
    "]\n",
    "\n",
    "allow_halftime = False\n",
    "\n",
    "# Result, can not be used for prediction\n",
    "results = ['FTHG', 'FTAG', 'FTR', 'HTHG', 'HTAG', 'HTR']\n",
    "\n",
    "# The value of these columns are not known before the match as well\n",
    "match_statistics = [\n",
    "    'Attendance', 'Referee', 'HS', 'AS', 'HST', 'AST', 'HHW', 'AHW', 'HC',\n",
    "    'AC', 'HF', 'AF', 'HFKC', 'AFKC', 'HO', 'AO', 'HY', 'AY', 'HR', 'AR',\n",
    "    'HBP', 'ABP'\n",
    "]\n",
    "\n",
    "X = df.drop(set(results + match_statistics +\n",
    "                ['Datetime', 'Season', 'HT', 'AT']).intersection(column_names),\n",
    "            axis=1)\n",
    "print(X.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting the datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import datetime, time\n",
    "\n",
    "\n",
    "# https://stackoverflow.com/questions/57330482/convert-data-frame-datatime-string-to-float-in-python-pandas\n",
    "def convertDate(dateString):\n",
    "    dateTime1 = datetime.datetime.strptime(dateString, '%Y-%m-%d %H:%M:%S')\n",
    "    #ignores time of the day\n",
    "    return int(time.mktime(dateTime1.timetuple())) // 86400\n",
    "\n",
    "\n",
    "def convertTime(timeString):\n",
    "    hour, minute = map(int, timeString.split(':'))\n",
    "    return hour * 60 + minute\n",
    "\n",
    "\n",
    "def reciprocal(x):\n",
    "    if x == 0:\n",
    "        return 0\n",
    "    return 1 / float(x)\n",
    "\n",
    "\n",
    "df['Date'] = df['Date'].apply(convertDate)\n",
    "df['Time'] = df['Time'].apply(convertTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Time</th>\n",
       "      <th>Datetime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18494</td>\n",
       "      <td>30</td>\n",
       "      <td>2020-08-21 00:30:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18494</td>\n",
       "      <td>30</td>\n",
       "      <td>2020-08-21 00:30:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18494</td>\n",
       "      <td>0</td>\n",
       "      <td>2020-08-21 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18494</td>\n",
       "      <td>0</td>\n",
       "      <td>2020-08-21 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18494</td>\n",
       "      <td>0</td>\n",
       "      <td>2020-08-21 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179802</th>\n",
       "      <td>11166</td>\n",
       "      <td>0</td>\n",
       "      <td>2000-07-29 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179803</th>\n",
       "      <td>11166</td>\n",
       "      <td>0</td>\n",
       "      <td>2000-07-29 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179804</th>\n",
       "      <td>11165</td>\n",
       "      <td>0</td>\n",
       "      <td>2000-07-28 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179805</th>\n",
       "      <td>11165</td>\n",
       "      <td>0</td>\n",
       "      <td>2000-07-28 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179806</th>\n",
       "      <td>11165</td>\n",
       "      <td>0</td>\n",
       "      <td>2000-07-28 00:00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>179803 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date  Time             Datetime\n",
       "0       18494    30  2020-08-21 00:30:00\n",
       "1       18494    30  2020-08-21 00:30:00\n",
       "2       18494     0  2020-08-21 00:00:00\n",
       "3       18494     0  2020-08-21 00:00:00\n",
       "4       18494     0  2020-08-21 00:00:00\n",
       "...       ...   ...                  ...\n",
       "179802  11166     0  2000-07-29 00:00:00\n",
       "179803  11166     0  2000-07-29 00:00:00\n",
       "179804  11165     0  2000-07-28 00:00:00\n",
       "179805  11165     0  2000-07-28 00:00:00\n",
       "179806  11165     0  2000-07-28 00:00:00\n",
       "\n",
       "[179803 rows x 3 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['Date','Time','Datetime']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fill the Nan in odds and calculate reciprocal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "# odds\n",
    "betting_odds = [\n",
    "    'B365H', 'B365D', 'B365A', 'BSH', 'BSD', 'BSA', 'BWH', 'BWD', 'BWA', 'GBH',\n",
    "    'GBD', 'GBA', 'IWH', 'IWD', 'IWA', 'LBH', 'LBD', 'LBA', 'PH', 'PD', 'PA',\n",
    "    'SOH', 'SOD', 'SOA', 'SBH', 'SBD', 'SBA', 'SJH', 'SJD', 'SJA', 'SYH',\n",
    "    'SYD', 'SYA', 'VCH', 'VCD', 'VCA', 'WHH', 'WHD', 'WHA', 'Bb1X2', 'BbMxH',\n",
    "    'BbAvH', 'BbMxD', 'BbAvD', 'BbMxA', 'BbAvA', 'MaxH', 'MaxD', 'MaxA',\n",
    "    'AvgH', 'AvgD', 'AvgA', 'B365CH', 'B365CD', 'B365CA', 'BWCH', 'BWCD',\n",
    "    'BWCA', 'IWCH', 'IWCD', 'IWCA', 'VCCH', 'VCCD', 'VCCA', 'WHCH', 'WHCD',\n",
    "    'WHCA', 'MaxCH', 'MaxCD', 'MaxCA', 'AvgCH', 'AvgCD', 'AvgCA', 'PSH', 'PSD',\n",
    "    'PSA', 'PSCA', 'PSCD'\n",
    "]\n",
    "\n",
    "# goals related to goals\n",
    "num_goals_odds = [\n",
    "    'BbMx>2.5', 'BbAv>2.5', 'BbMx<2.5', 'BbAv<2.5', 'GB>2.5', 'GB<2.5',\n",
    "    'B365>2.5', 'B365<2.5', 'P>2.5', 'P<2.5', 'Max>2.5', 'Max<2.5', 'Avg>2.5',\n",
    "    'Avg<2.5', 'B365C>2.5', 'B365C<2.5', 'PC>2.5', 'PC<2.5', 'MaxC>2.5',\n",
    "    'MaxC<2.5', 'AvgC>2.5', 'AvgC<2.5'\n",
    "]\n",
    "\n",
    "# odds in asia\n",
    "asian_odds = [\n",
    "    'AHh', 'BbMxAHH', 'BbAvAHH', 'BbMxAHA', 'BbAvAHA', 'GBAHH', 'GBAHA',\n",
    "    'GBAH', 'LBAHH', 'LBAHA', 'LBAH', 'B365AHH', 'B365AHA', 'B365AH', 'PAHH',\n",
    "    'PAHA', 'MaxAHH', 'MaxAHA', 'AvgAHH', 'AvgAHA', 'B365CAHH', 'B365CAHA',\n",
    "    'PCAHH', 'PCAHA', 'MaxCAHH', 'MaxCAHA', 'AvgCAHH', 'AvgCAHA'\n",
    "]\n",
    "\n",
    "# Fill the NA with the mean\n",
    "for i in betting_odds:\n",
    "    try:\n",
    "        df[i].fillna((df[i].mean()), inplace=True)\n",
    "    except:\n",
    "        print(i)\n",
    "\n",
    "for i in num_goals_odds + asian_odds:\n",
    "    df[i].fillna((df[i].mean()), inplace=True)\n",
    "\n",
    "column_names = list(df.columns.values)\n",
    "\n",
    "#Replace empty values with -1\n",
    "for i in column_names:\n",
    "    df[i].fillna(-1, inplace=True)\n",
    "\n",
    "# convert the fraction to float(1/x->x)\n",
    "for col in betting_odds + num_goals_odds + asian_odds:\n",
    "    try:\n",
    "        df[col] = df[col].apply(reciprocal)\n",
    "    except KeyError:\n",
    "        continue\n",
    "        #print(col, \"KeyError\")\n",
    "    except:\n",
    "        print(col)\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean BbAHh columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "BbAHh = pd.DataFrame(df[ df['BbAHh'].notnull()]['BbAHh'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BbAHh</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>111324</th>\n",
       "      <td>+0.5,+1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111325</th>\n",
       "      <td>-1,-1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111326</th>\n",
       "      <td>0,+0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111327</th>\n",
       "      <td>-1,-1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111330</th>\n",
       "      <td>+0.5,+1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111331</th>\n",
       "      <td>-1,-1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111336</th>\n",
       "      <td>-0.5,-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111341</th>\n",
       "      <td>-0.5,-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111342</th>\n",
       "      <td>0,-0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111343</th>\n",
       "      <td>-1.5,-2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          BbAHh\n",
       "111324  +0.5,+1\n",
       "111325  -1,-1.5\n",
       "111326   0,+0.5\n",
       "111327  -1,-1.5\n",
       "111330  +0.5,+1\n",
       "111331  -1,-1.5\n",
       "111336  -0.5,-1\n",
       "111341  -0.5,-1\n",
       "111342   0,-0.5\n",
       "111343  -1.5,-2"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vals = pd.to_numeric(BbAHh['BbAHh'],errors='coerce').notna()\n",
    "BbAHh[[not item for item in vals]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final dataframe shape: (179803, 172)\n",
      "      Season             Datetime  Div   Country          League   Referee  \\\n",
      "0       2020  2020-08-21 00:30:00  USA       USA             MLS        -1   \n",
      "1       2020  2020-08-21 00:30:00  USA       USA             MLS        -1   \n",
      "2       2020  2020-08-21 00:00:00  BRA    Brazil         Serie A        -1   \n",
      "3       2020  2020-08-21 00:00:00  USA       USA             MLS        -1   \n",
      "4       2020  2020-08-21 00:00:00  BRA    Brazil         Serie A        -1   \n",
      "5       2020  2020-08-20 23:15:00  BRA    Brazil         Serie A        -1   \n",
      "6  2020/2021  2020-08-20 19:30:00  SC0  Scotland  Premier League  W Collum   \n",
      "7       2020  2020-08-20 13:00:00  CHN     China    Super League        -1   \n",
      "8       2020  2020-08-20 11:00:00  CHN     China    Super League        -1   \n",
      "9       2020  2020-08-20 01:30:00  BRA    Brazil         Serie A        -1   \n",
      "\n",
      "                 HomeTeam            AwayTeam  FTHG  FTAG FTR  HTHG  HTAG HTR  \\\n",
      "0           Columbus Crew        Chicago Fire     3     0   H    -1    -1  -1   \n",
      "1  New England Revolution  Philadelphia Union     0     0   D    -1    -1  -1   \n",
      "2                   Ceara               Vasco     0     3   A    -1    -1  -1   \n",
      "3      New York Red Bulls       New York City     1     0   H    -1    -1  -1   \n",
      "4               Sao Paulo               Bahia     1     1   D    -1    -1  -1   \n",
      "5            Sport Recife              Santos     0     1   A    -1    -1  -1   \n",
      "6            St Johnstone            Aberdeen     0     1   A     0     0   D   \n",
      "7                Shenzhen          Dalian Pro     3     2   H    -1    -1  -1   \n",
      "8    Guangzhou Evergrande      Jiangsu Suning     2     1   H    -1    -1  -1   \n",
      "9             Corinthians            Coritiba     3     1   H    -1    -1  -1   \n",
      "\n",
      "        PSH      PSD       PSA     B365H     B365D     B365A       LBH  \\\n",
      "0  2.626553  3.82779  4.321046  2.432321  3.579836  4.089169  2.336214   \n",
      "1  2.626553  3.82779  4.321046  2.432321  3.579836  4.089169  2.336214   \n",
      "2  2.626553  3.82779  4.321046  2.432321  3.579836  4.089169  2.336214   \n",
      "3  2.626553  3.82779  4.321046  2.432321  3.579836  4.089169  2.336214   \n",
      "4  2.626553  3.82779  4.321046  2.432321  3.579836  4.089169  2.336214   \n",
      "5  2.626553  3.82779  4.321046  2.432321  3.579836  4.089169  2.336214   \n",
      "6  2.626553  3.82779  4.321046  2.550000  3.100000  2.870000  2.336214   \n",
      "7  2.626553  3.82779  4.321046  2.432321  3.579836  4.089169  2.336214   \n",
      "8  2.626553  3.82779  4.321046  2.432321  3.579836  4.089169  2.336214   \n",
      "9  2.626553  3.82779  4.321046  2.432321  3.579836  4.089169  2.336214   \n",
      "\n",
      "        LBD      LBA       BWH       BWD       BWA  ABP   AC    AF  AFKC  \\\n",
      "0  3.460465  3.91564  2.397433  3.512485  3.956345 -1.0 -1.0  -1.0  -1.0   \n",
      "1  3.460465  3.91564  2.397433  3.512485  3.956345 -1.0 -1.0  -1.0  -1.0   \n",
      "2  3.460465  3.91564  2.397433  3.512485  3.956345 -1.0 -1.0  -1.0  -1.0   \n",
      "3  3.460465  3.91564  2.397433  3.512485  3.956345 -1.0 -1.0  -1.0  -1.0   \n",
      "4  3.460465  3.91564  2.397433  3.512485  3.956345 -1.0 -1.0  -1.0  -1.0   \n",
      "5  3.460465  3.91564  2.397433  3.512485  3.956345 -1.0 -1.0  -1.0  -1.0   \n",
      "6  3.460465  3.91564  2.600000  3.100000  2.750000 -1.0  1.0  13.0  -1.0   \n",
      "7  3.460465  3.91564  2.397433  3.512485  3.956345 -1.0 -1.0  -1.0  -1.0   \n",
      "8  3.460465  3.91564  2.397433  3.512485  3.956345 -1.0 -1.0  -1.0  -1.0   \n",
      "9  3.460465  3.91564  2.397433  3.512485  3.956345 -1.0 -1.0  -1.0  -1.0   \n",
      "\n",
      "   AHCh  AHW       AHh   AO   AR   AS  AST  AT   AY  Attendance   Avg<2.5  \\\n",
      "0  -1.0 -1.0 -0.283365 -1.0 -1.0 -1.0 -1.0  -1 -1.0        -1.0  1.959665   \n",
      "1  -1.0 -1.0 -0.283365 -1.0 -1.0 -1.0 -1.0  -1 -1.0        -1.0  1.959665   \n",
      "2  -1.0 -1.0 -0.283365 -1.0 -1.0 -1.0 -1.0  -1 -1.0        -1.0  1.959665   \n",
      "3  -1.0 -1.0 -0.283365 -1.0 -1.0 -1.0 -1.0  -1 -1.0        -1.0  1.959665   \n",
      "4  -1.0 -1.0 -0.283365 -1.0 -1.0 -1.0 -1.0  -1 -1.0        -1.0  1.959665   \n",
      "5  -1.0 -1.0 -0.283365 -1.0 -1.0 -1.0 -1.0  -1 -1.0        -1.0  1.959665   \n",
      "6   0.0 -1.0  0.000000 -1.0  0.0  6.0  3.0  -1  3.0        -1.0  1.490000   \n",
      "7  -1.0 -1.0 -0.283365 -1.0 -1.0 -1.0 -1.0  -1 -1.0        -1.0  1.959665   \n",
      "8  -1.0 -1.0 -0.283365 -1.0 -1.0 -1.0 -1.0  -1 -1.0        -1.0  1.959665   \n",
      "9  -1.0 -1.0 -0.283365 -1.0 -1.0 -1.0 -1.0  -1 -1.0        -1.0  1.959665   \n",
      "\n",
      "   Avg>2.5  AvgA    AvgAHA    AvgAHH  AvgC<2.5  AvgC>2.5     AvgCA   AvgCAHA  \\\n",
      "0   1.9369  4.34  1.914502  1.925411   1.97245  1.946943  4.067782  1.924126   \n",
      "1   1.9369  3.07  1.914502  1.925411   1.97245  1.946943  4.067782  1.924126   \n",
      "2   1.9369  3.92  1.914502  1.925411   1.97245  1.946943  4.067782  1.924126   \n",
      "3   1.9369  2.49  1.914502  1.925411   1.97245  1.946943  4.067782  1.924126   \n",
      "4   1.9369  5.13  1.914502  1.925411   1.97245  1.946943  4.067782  1.924126   \n",
      "5   1.9369  2.93  1.914502  1.925411   1.97245  1.946943  4.067782  1.924126   \n",
      "6   2.5800  2.83  2.010000  1.840000   1.45000  2.740000  2.640000  1.830000   \n",
      "7   1.9369  2.13  1.914502  1.925411   1.97245  1.946943  4.067782  1.924126   \n",
      "8   1.9369  3.62  1.914502  1.925411   1.97245  1.946943  4.067782  1.924126   \n",
      "9   1.9369  5.00  1.914502  1.925411   1.97245  1.946943  4.067782  1.924126   \n",
      "\n",
      "    AvgCAHH     AvgCD     AvgCH  AvgD  AvgH  B365<2.5  B365>2.5   B365AH  \\\n",
      "0  1.923034  3.757013  2.608919  3.62  1.81  1.925524  1.927336 -0.36354   \n",
      "1  1.923034  3.757013  2.608919  3.62  2.18  1.925524  1.927336 -0.36354   \n",
      "2  1.923034  3.757013  2.608919  3.07  2.07  1.925524  1.927336 -0.36354   \n",
      "3  1.923034  3.757013  2.608919  3.53  2.65  1.925524  1.927336 -0.36354   \n",
      "4  1.923034  3.757013  2.608919  3.41  1.74  1.925524  1.927336 -0.36354   \n",
      "5  1.923034  3.757013  2.608919  3.15  2.45  1.925524  1.927336 -0.36354   \n",
      "6  2.020000  2.950000  2.890000  3.07  2.58  1.530000  2.500000 -0.36354   \n",
      "7  1.923034  3.757013  2.608919  3.49  3.13  1.925524  1.927336 -0.36354   \n",
      "8  1.923034  3.757013  2.608919  3.38  2.01  1.925524  1.927336 -0.36354   \n",
      "9  1.923034  3.757013  2.608919  3.14  1.83  1.925524  1.927336 -0.36354   \n",
      "\n",
      "   B365AHA   B365AHH  B365C<2.5  B365C>2.5    B365CA  B365CAHA  B365CAHH  \\\n",
      "0  1.93377  1.937684   1.971409   1.950857  4.111967  1.934048     1.934   \n",
      "1  1.93377  1.937684   1.971409   1.950857  4.111967  1.934048     1.934   \n",
      "2  1.93377  1.937684   1.971409   1.950857  4.111967  1.934048     1.934   \n",
      "3  1.93377  1.937684   1.971409   1.950857  4.111967  1.934048     1.934   \n",
      "4  1.93377  1.937684   1.971409   1.950857  4.111967  1.934048     1.934   \n",
      "5  1.93377  1.937684   1.971409   1.950857  4.111967  1.934048     1.934   \n",
      "6  1.93377  1.937684   1.400000   3.000000  2.550000  1.800000     2.050   \n",
      "7  1.93377  1.937684   1.971409   1.950857  4.111967  1.934048     1.934   \n",
      "8  1.93377  1.937684   1.971409   1.950857  4.111967  1.934048     1.934   \n",
      "9  1.93377  1.937684   1.971409   1.950857  4.111967  1.934048     1.934   \n",
      "\n",
      "     B365CD    B365CH       BSA       BSD      BSH      BWCA     BWCD  \\\n",
      "0  3.806783  2.617248  3.896264  3.453497  2.33046  4.055368  3.75014   \n",
      "1  3.806783  2.617248  3.896264  3.453497  2.33046  4.055368  3.75014   \n",
      "2  3.806783  2.617248  3.896264  3.453497  2.33046  4.055368  3.75014   \n",
      "3  3.806783  2.617248  3.896264  3.453497  2.33046  4.055368  3.75014   \n",
      "4  3.806783  2.617248  3.896264  3.453497  2.33046  4.055368  3.75014   \n",
      "5  3.806783  2.617248  3.896264  3.453497  2.33046  4.055368  3.75014   \n",
      "6  3.100000  2.870000  3.896264  3.453497  2.33046  2.700000  3.10000   \n",
      "7  3.806783  2.617248  3.896264  3.453497  2.33046  4.055368  3.75014   \n",
      "8  3.806783  2.617248  3.896264  3.453497  2.33046  4.055368  3.75014   \n",
      "9  3.806783  2.617248  3.896264  3.453497  2.33046  4.055368  3.75014   \n",
      "\n",
      "       BWCH     Bb1X2 BbAH  BbAHh  BbAv<2.5  BbAv>2.5     BbAvA   BbAvAHA  \\\n",
      "0  2.591782  38.38119   -1   -1.0  1.864361  1.949059  4.015791  1.974384   \n",
      "1  2.591782  38.38119   -1   -1.0  1.864361  1.949059  4.015791  1.974384   \n",
      "2  2.591782  38.38119   -1   -1.0  1.864361  1.949059  4.015791  1.974384   \n",
      "3  2.591782  38.38119   -1   -1.0  1.864361  1.949059  4.015791  1.974384   \n",
      "4  2.591782  38.38119   -1   -1.0  1.864361  1.949059  4.015791  1.974384   \n",
      "5  2.591782  38.38119   -1   -1.0  1.864361  1.949059  4.015791  1.974384   \n",
      "6  2.650000  38.38119   -1   -1.0  1.864361  1.949059  4.015791  1.974384   \n",
      "7  2.591782  38.38119   -1   -1.0  1.864361  1.949059  4.015791  1.974384   \n",
      "8  2.591782  38.38119   -1   -1.0  1.864361  1.949059  4.015791  1.974384   \n",
      "9  2.591782  38.38119   -1   -1.0  1.864361  1.949059  4.015791  1.974384   \n",
      "\n",
      "    BbAvAHH     BbAvD     BbAvH  BbMx<2.5  BbMx>2.5     BbMxA   BbMxAHA  \\\n",
      "0  1.915468  3.523941  2.422323  1.963087  2.058638  4.597447  2.051056   \n",
      "1  1.915468  3.523941  2.422323  1.963087  2.058638  4.597447  2.051056   \n",
      "2  1.915468  3.523941  2.422323  1.963087  2.058638  4.597447  2.051056   \n",
      "3  1.915468  3.523941  2.422323  1.963087  2.058638  4.597447  2.051056   \n",
      "4  1.915468  3.523941  2.422323  1.963087  2.058638  4.597447  2.051056   \n",
      "5  1.915468  3.523941  2.422323  1.963087  2.058638  4.597447  2.051056   \n",
      "6  1.915468  3.523941  2.422323  1.963087  2.058638  4.597447  2.051056   \n",
      "7  1.915468  3.523941  2.422323  1.963087  2.058638  4.597447  2.051056   \n",
      "8  1.915468  3.523941  2.422323  1.963087  2.058638  4.597447  2.051056   \n",
      "9  1.915468  3.523941  2.422323  1.963087  2.058638  4.597447  2.051056   \n",
      "\n",
      "    BbMxAHH     BbMxD     BbMxH  BbOU   Date    GB<2.5    GB>2.5       GBA  \\\n",
      "0  1.975116  3.795117  2.617393  -1.0  18494  1.847058  1.835195  3.923952   \n",
      "1  1.975116  3.795117  2.617393  -1.0  18494  1.847058  1.835195  3.923952   \n",
      "2  1.975116  3.795117  2.617393  -1.0  18494  1.847058  1.835195  3.923952   \n",
      "3  1.975116  3.795117  2.617393  -1.0  18494  1.847058  1.835195  3.923952   \n",
      "4  1.975116  3.795117  2.617393  -1.0  18494  1.847058  1.835195  3.923952   \n",
      "5  1.975116  3.795117  2.617393  -1.0  18493  1.847058  1.835195  3.923952   \n",
      "6  1.975116  3.795117  2.617393  -1.0  18493  1.847058  1.835195  3.923952   \n",
      "7  1.975116  3.795117  2.617393  -1.0  18493  1.847058  1.835195  3.923952   \n",
      "8  1.975116  3.795117  2.617393  -1.0  18493  1.847058  1.835195  3.923952   \n",
      "9  1.975116  3.795117  2.617393  -1.0  18493  1.847058  1.835195  3.923952   \n",
      "\n",
      "       GBAH     GBAHA     GBAHH       GBD       GBH  HBP   HC    HF  HFKC  \\\n",
      "0 -0.363313  1.844581  1.908906  3.390817  2.278375 -1.0 -1.0  -1.0  -1.0   \n",
      "1 -0.363313  1.844581  1.908906  3.390817  2.278375 -1.0 -1.0  -1.0  -1.0   \n",
      "2 -0.363313  1.844581  1.908906  3.390817  2.278375 -1.0 -1.0  -1.0  -1.0   \n",
      "3 -0.363313  1.844581  1.908906  3.390817  2.278375 -1.0 -1.0  -1.0  -1.0   \n",
      "4 -0.363313  1.844581  1.908906  3.390817  2.278375 -1.0 -1.0  -1.0  -1.0   \n",
      "5 -0.363313  1.844581  1.908906  3.390817  2.278375 -1.0 -1.0  -1.0  -1.0   \n",
      "6 -0.363313  1.844581  1.908906  3.390817  2.278375 -1.0  5.0  17.0  -1.0   \n",
      "7 -0.363313  1.844581  1.908906  3.390817  2.278375 -1.0 -1.0  -1.0  -1.0   \n",
      "8 -0.363313  1.844581  1.908906  3.390817  2.278375 -1.0 -1.0  -1.0  -1.0   \n",
      "9 -0.363313  1.844581  1.908906  3.390817  2.278375 -1.0 -1.0  -1.0  -1.0   \n",
      "\n",
      "   HHW   HO   HR    HS  HST  HT   HY       IWA      IWCA      IWCD     IWCH  \\\n",
      "0 -1.0 -1.0 -1.0  -1.0 -1.0  -1 -1.0  3.729131  3.914262  3.661487  2.56343   \n",
      "1 -1.0 -1.0 -1.0  -1.0 -1.0  -1 -1.0  3.729131  3.914262  3.661487  2.56343   \n",
      "2 -1.0 -1.0 -1.0  -1.0 -1.0  -1 -1.0  3.729131  3.914262  3.661487  2.56343   \n",
      "3 -1.0 -1.0 -1.0  -1.0 -1.0  -1 -1.0  3.729131  3.914262  3.661487  2.56343   \n",
      "4 -1.0 -1.0 -1.0  -1.0 -1.0  -1 -1.0  3.729131  3.914262  3.661487  2.56343   \n",
      "5 -1.0 -1.0 -1.0  -1.0 -1.0  -1 -1.0  3.729131  3.914262  3.661487  2.56343   \n",
      "6 -1.0 -1.0  0.0  11.0  1.0  -1  1.0  2.850000  2.650000  2.800000  2.90000   \n",
      "7 -1.0 -1.0 -1.0  -1.0 -1.0  -1 -1.0  3.729131  3.914262  3.661487  2.56343   \n",
      "8 -1.0 -1.0 -1.0  -1.0 -1.0  -1 -1.0  3.729131  3.914262  3.661487  2.56343   \n",
      "9 -1.0 -1.0 -1.0  -1.0 -1.0  -1 -1.0  3.729131  3.914262  3.661487  2.56343   \n",
      "\n",
      "        IWD       IWH      LBAH    LBAHA     LBAHH   Max<2.5   Max>2.5  MaxA  \\\n",
      "0  3.363963  2.304622 -0.358449  1.92445  1.920873  2.050356  2.020563  4.65   \n",
      "1  3.363963  2.304622 -0.358449  1.92445  1.920873  2.050356  2.020563  3.21   \n",
      "2  3.363963  2.304622 -0.358449  1.92445  1.920873  2.050356  2.020563  4.30   \n",
      "3  3.363963  2.304622 -0.358449  1.92445  1.920873  2.050356  2.020563  2.60   \n",
      "4  3.363963  2.304622 -0.358449  1.92445  1.920873  2.050356  2.020563  5.90   \n",
      "5  3.363963  2.304622 -0.358449  1.92445  1.920873  2.050356  2.020563  3.13   \n",
      "6  2.950000  2.600000 -0.358449  1.92445  1.920873  1.560000  2.770000  3.04   \n",
      "7  3.363963  2.304622 -0.358449  1.92445  1.920873  2.050356  2.020563  2.27   \n",
      "8  3.363963  2.304622 -0.358449  1.92445  1.920873  2.050356  2.020563  4.20   \n",
      "9  3.363963  2.304622 -0.358449  1.92445  1.920873  2.050356  2.020563  5.65   \n",
      "\n",
      "    MaxAHA    MaxAHH  MaxC<2.5  MaxC>2.5     MaxCA   MaxCAHA   MaxCAHH  \\\n",
      "0  1.97998  1.990138  2.079833  2.044024  4.578191  2.002175  2.000659   \n",
      "1  1.97998  1.990138  2.079833  2.044024  4.578191  2.002175  2.000659   \n",
      "2  1.97998  1.990138  2.079833  2.044024  4.578191  2.002175  2.000659   \n",
      "3  1.97998  1.990138  2.079833  2.044024  4.578191  2.002175  2.000659   \n",
      "4  1.97998  1.990138  2.079833  2.044024  4.578191  2.002175  2.000659   \n",
      "5  1.97998  1.990138  2.079833  2.044024  4.578191  2.002175  2.000659   \n",
      "6  2.09000  1.910000  1.520000  3.000000  2.770000  1.920000  2.110000   \n",
      "7  1.97998  1.990138  2.079833  2.044024  4.578191  2.002175  2.000659   \n",
      "8  1.97998  1.990138  2.079833  2.044024  4.578191  2.002175  2.000659   \n",
      "9  1.97998  1.990138  2.079833  2.044024  4.578191  2.002175  2.000659   \n",
      "\n",
      "      MaxCD     MaxCH  MaxD  MaxH     P<2.5     P>2.5        PA     PAHA  \\\n",
      "0  4.018165  2.816365  3.80  1.89  2.007932  1.975706  4.420000  1.93761   \n",
      "1  4.018165  2.816365  3.75  2.30  2.007932  1.975706  3.150000  1.93761   \n",
      "2  4.018165  2.816365  3.36  2.15  2.007932  1.975706  4.080000  1.93761   \n",
      "3  4.018165  2.816365  3.75  2.90  2.007932  1.975706  2.550000  1.93761   \n",
      "4  4.018165  2.816365  3.64  1.87  2.007932  1.975706  5.220000  1.93761   \n",
      "5  4.018165  2.816365  3.46  2.65  2.007932  1.975706  2.980000  1.93761   \n",
      "6  3.160000  3.080000  3.24  2.75  2.007932  1.975706  4.315414  1.93761   \n",
      "7  4.018165  2.816365  3.66  3.52  2.007932  1.975706  2.240000  1.93761   \n",
      "8  4.018165  2.816365  3.72  2.18  2.007932  1.975706  3.630000  1.93761   \n",
      "9  4.018165  2.816365  3.50  1.92  2.007932  1.975706  5.120000  1.93761   \n",
      "\n",
      "       PAHH    PC<2.5    PC>2.5     PCAHA     PCAHH        PD        PH  \\\n",
      "0  1.951992  2.026772  1.996904  1.952549  1.952718  3.610000  1.870000   \n",
      "1  1.951992  2.026772  1.996904  1.952549  1.952718  3.720000  2.210000   \n",
      "2  1.951992  2.026772  1.996904  1.952549  1.952718  3.150000  2.070000   \n",
      "3  1.951992  2.026772  1.996904  1.952549  1.952718  3.640000  2.690000   \n",
      "4  1.951992  2.026772  1.996904  1.952549  1.952718  3.580000  1.740000   \n",
      "5  1.951992  2.026772  1.996904  1.952549  1.952718  3.260000  2.480000   \n",
      "6  1.951992  1.430000  2.930000  1.850000  2.050000  3.797963  2.536016   \n",
      "7  1.951992  2.026772  1.996904  1.952549  1.952718  3.640000  3.080000   \n",
      "8  1.951992  2.026772  1.996904  1.952549  1.952718  3.390000  2.100000   \n",
      "9  1.951992  2.026772  1.996904  1.952549  1.952718  3.170000  1.870000   \n",
      "\n",
      "       PSCA      PSCD      PSCH       SBA       SBD      SBH       SJA  \\\n",
      "0  4.413415  3.866584 -1.000000  3.934713  3.396861  2.27063  4.048946   \n",
      "1  4.413415  3.866584 -1.000000  3.934713  3.396861  2.27063  4.048946   \n",
      "2  4.413415  3.866584 -1.000000  3.934713  3.396861  2.27063  4.048946   \n",
      "3  4.413415  3.866584 -1.000000  3.934713  3.396861  2.27063  4.048946   \n",
      "4  4.413415  3.866584 -1.000000  3.934713  3.396861  2.27063  4.048946   \n",
      "5  4.413415  3.866584 -1.000000  3.934713  3.396861  2.27063  4.048946   \n",
      "6  2.710000  2.970000  0.334448  3.934713  3.396861  2.27063  4.048946   \n",
      "7  4.413415  3.866584 -1.000000  3.934713  3.396861  2.27063  4.048946   \n",
      "8  4.413415  3.866584 -1.000000  3.934713  3.396861  2.27063  4.048946   \n",
      "9  4.413415  3.866584 -1.000000  3.934713  3.396861  2.27063  4.048946   \n",
      "\n",
      "        SJD       SJH       SOA       SOD       SOH       SYA       SYD  \\\n",
      "0  3.470696  2.346414  3.826179  3.381151  2.205424  3.832744  3.393323   \n",
      "1  3.470696  2.346414  3.826179  3.381151  2.205424  3.832744  3.393323   \n",
      "2  3.470696  2.346414  3.826179  3.381151  2.205424  3.832744  3.393323   \n",
      "3  3.470696  2.346414  3.826179  3.381151  2.205424  3.832744  3.393323   \n",
      "4  3.470696  2.346414  3.826179  3.381151  2.205424  3.832744  3.393323   \n",
      "5  3.470696  2.346414  3.826179  3.381151  2.205424  3.832744  3.393323   \n",
      "6  3.470696  2.346414  3.826179  3.381151  2.205424  3.832744  3.393323   \n",
      "7  3.470696  2.346414  3.826179  3.381151  2.205424  3.832744  3.393323   \n",
      "8  3.470696  2.346414  3.826179  3.381151  2.205424  3.832744  3.393323   \n",
      "9  3.470696  2.346414  3.826179  3.381151  2.205424  3.832744  3.393323   \n",
      "\n",
      "        SYH  Time       VCA      VCCA      VCCD      VCCH       VCD       VCH  \\\n",
      "0  2.137174    30  4.162608  4.156753  3.810942  2.650968  3.621904  2.470562   \n",
      "1  2.137174    30  4.162608  4.156753  3.810942  2.650968  3.621904  2.470562   \n",
      "2  2.137174     0  4.162608  4.156753  3.810942  2.650968  3.621904  2.470562   \n",
      "3  2.137174     0  4.162608  4.156753  3.810942  2.650968  3.621904  2.470562   \n",
      "4  2.137174     0  4.162608  4.156753  3.810942  2.650968  3.621904  2.470562   \n",
      "5  2.137174  1395  4.162608  4.156753  3.810942  2.650968  3.621904  2.470562   \n",
      "6  2.137174  1170  2.880000  2.600000  3.000000  3.000000  3.100000  2.600000   \n",
      "7  2.137174   780  4.162608  4.156753  3.810942  2.650968  3.621904  2.470562   \n",
      "8  2.137174   660  4.162608  4.156753  3.810942  2.650968  3.621904  2.470562   \n",
      "9  2.137174    90  4.162608  4.156753  3.810942  2.650968  3.621904  2.470562   \n",
      "\n",
      "        WHA      WHCA      WHCD      WHCH       WHD       WHH  \n",
      "0  3.944091  4.205906  3.771464  2.634679  3.466449  2.374814  \n",
      "1  3.944091  4.205906  3.771464  2.634679  3.466449  2.374814  \n",
      "2  3.944091  4.205906  3.771464  2.634679  3.466449  2.374814  \n",
      "3  3.944091  4.205906  3.771464  2.634679  3.466449  2.374814  \n",
      "4  3.944091  4.205906  3.771464  2.634679  3.466449  2.374814  \n",
      "5  3.944091  4.205906  3.771464  2.634679  3.466449  2.374814  \n",
      "6  2.880000  2.600000  3.000000  2.900000  3.100000  2.550000  \n",
      "7  3.944091  4.205906  3.771464  2.634679  3.466449  2.374814  \n",
      "8  3.944091  4.205906  3.771464  2.634679  3.466449  2.374814  \n",
      "9  3.944091  4.205906  3.771464  2.634679  3.466449  2.374814  \n"
     ]
    }
   ],
   "source": [
    "def conv_BbAHh(i):\n",
    "    try:\n",
    "        return float(i)\n",
    "    except:\n",
    "        # print(i)\n",
    "        a, b = map(float, i.split(','))\n",
    "        # print(a, b)\n",
    "        return (a + b) / 2\n",
    "\n",
    "\n",
    "try:\n",
    "    df['BbAHh'] = df['BbAHh'].apply(conv_BbAHh)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "print(\"Final dataframe shape:\", df.shape)\n",
    "print(df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build DF without time(for ML Method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the features of ML Model, we use the odds to predict the probability of the game result and accroding to the following formula:\n",
    "\n",
    "\n",
    "We can calculate the final odds.\n",
    "\n",
    "So the features for prediction is without time sequence and the desired result is the final result of the game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(179803, 138)\n",
      "(179803, 177)\n"
     ]
    }
   ],
   "source": [
    "ML_X = df.drop(\n",
    "    set(results + match_statistics +\n",
    "        ['Date','Time','Datetime', 'Season', 'HT', 'AT']).intersection(column_names),\n",
    "    axis=1)\n",
    "print(ML_X.shape)\n",
    "\n",
    "# ground truth(match result)\n",
    "ML_Y = df['FTR']\n",
    "\n",
    "#\n",
    "ce_binaryX = ce.BinaryEncoder(\n",
    "    cols=['HomeTeam', 'AwayTeam', 'Div', 'League', 'Country'])\n",
    "ohe = ce.OneHotEncoder(cols=[])\n",
    "ML_X = ce_binaryX.fit_transform(ML_X)\n",
    "ML_X = ohe.fit_transform(ML_X)\n",
    "\n",
    "# Encode the string columns\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(['H', 'D', 'A'])\n",
    "\n",
    "ML_Y = label_encoder.fit_transform(ML_Y)\n",
    "\n",
    "print(ML_X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build DF with time(for LSTM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X: Features with time sequence\n",
    "\n",
    "Y: odds of B365\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(179803, 140)\n",
      "(179803, 179)\n"
     ]
    }
   ],
   "source": [
    "seq_X = df.drop(\n",
    "    set(results + match_statistics +\n",
    "        ['Datetime', 'Season', 'HT', 'AT']).intersection(column_names),\n",
    "    axis=1)\n",
    "print(seq_X.shape)\n",
    "\n",
    "# ground truth:odds\n",
    "seq_Y = df['B365H']\n",
    "\n",
    "#\n",
    "ce_binaryX = ce.BinaryEncoder(\n",
    "    cols=['HomeTeam', 'AwayTeam', 'Div', 'League', 'Country'])\n",
    "\n",
    "ohe = ce.OneHotEncoder(cols=[])\n",
    "seq_X = ce_binaryX.fit_transform(seq_X)\n",
    "seq_X = ohe.fit_transform(seq_X)\n",
    "\n",
    "print(seq_X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TBD(BELOW THIS LINE)--------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then proceed to the unsupervised learning section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 153\n",
    "\n",
    "# 80-20 split on training and testing data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import SpectralClustering\n",
    "import matplotlib.pyplot as plt\n",
    "from hmmlearn import hmm # Hidden Markovnikov model\n",
    "\n",
    "scaler = StandardScaler()\n",
    "pca = PCA()\n",
    "pca10 = PCA(n_components=10)\n",
    "pipeline = make_pipeline(scaler,pca)\n",
    "pipeline10 = make_pipeline(scaler,pca10)\n",
    "\n",
    "pipeline.fit(X_train)\n",
    "pipeline10.fit(X_train)\n",
    "\n",
    "# Plot the explained variances of the first 10 components\n",
    "features = range(pca10.n_components_)\n",
    "plt.bar(features, pca10.explained_variance_)\n",
    "plt.xlabel('PCA feature')\n",
    "plt.ylabel('variance')\n",
    "plt.xticks(features)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out that from the graph above, only 4 components of the PCA reduction have significant variance. Hence, we will use 4 dimensions in our reduced dimensionality space. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We tried to scale the input and fit the data but to no avail. The clustering turns out to be unable to differentiate between wins, draws and losses. With regards to the score, kmeans with PCA and normaliser is the best, even though it is still a poor fit since the score is extremely negative (0 being the best)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shown above that only first 4 PCA features have significant variance\n",
    "pca = PCA(n_components=4)\n",
    "scaler = StandardScaler()\n",
    "normalizer = Normalizer()\n",
    "kmeans = KMeans(n_clusters=3)\n",
    "\n",
    "kpipeline = make_pipeline(pca,kmeans)\n",
    "spipeline = make_pipeline(pca,scaler,kmeans)\n",
    "npipeline = make_pipeline(pca,normalizer,kmeans)\n",
    "\n",
    "kmeans.fit(X_train)\n",
    "kpipeline.fit(X_train)\n",
    "spipeline.fit(X_train)\n",
    "npipeline.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=4)\n",
    "scaler = StandardScaler()\n",
    "normalizer = Normalizer()\n",
    "#sc = SpectralClustering(3, assign_labels='discretize')\n",
    "\n",
    "scpipeline = make_pipeline(pca,sc)\n",
    "sscpipeline = make_pipeline(pca,scaler,sc)\n",
    "nscpipeline = make_pipeline(pca,normalizer,sc)\n",
    "\n",
    "scpipeline.fit(X_train)\n",
    "sscpipeline.fit(X_train)\n",
    "nscpipeline.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=4)\n",
    "normalizer = Normalizer()\n",
    "gm = hmm.GaussianHMM(n_components=3)\n",
    "\n",
    "ngmpipeline = make_pipeline(pca,normalizer,gm)\n",
    "\n",
    "ngmpipeline.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The presence of PCA improves raw performance by reducing the number of computations for features with low variance\n",
    "\n",
    "print(\"Outcome Legend (Unsupervised Learning)\")\n",
    "print(\"0: Away win\")\n",
    "print(\"1: Draw\")\n",
    "print(\"2: Home win\")\n",
    "print(\"Note: Supervised learning uses a different outcome legend\\n\")\n",
    "\n",
    "kmeans.fit(X_train, y_train)\n",
    "labels = kmeans.predict(X_test)\n",
    "df1 = pd.DataFrame({'labels': labels, 'wins': y_test})\n",
    "ct = pd.crosstab(df1['labels'],df1['wins'])\n",
    "print('kmeans only')\n",
    "print(ct)\n",
    "print(\"Score:\", kmeans.score(X_test, y_test))\n",
    "print()\n",
    "\n",
    "kpipeline.fit(X_train, y_train)\n",
    "labels = kpipeline.predict(X_test)\n",
    "df1 = pd.DataFrame({'labels': labels, 'wins': y_test})\n",
    "ct = pd.crosstab(df1['labels'],df1['wins'])\n",
    "print('kmeans with PCA')\n",
    "print(ct)\n",
    "print(\"Score:\", kpipeline.score(X_test, y_test))\n",
    "print()\n",
    "\n",
    "spipeline.fit(X_train, y_train)\n",
    "labels = spipeline.predict(X_test)\n",
    "df1 = pd.DataFrame({'labels': labels, 'wins': y_test})\n",
    "ct = pd.crosstab(df1['labels'],df1['wins'])\n",
    "print('kmeans with PCA & scaler')\n",
    "print(ct)\n",
    "print(\"Score:\", spipeline.score(X_test, y_test))\n",
    "print()\n",
    "\n",
    "npipeline.fit(X_train, y_train)\n",
    "labels = npipeline.predict(X_test)\n",
    "df1 = pd.DataFrame({'labels': labels, 'wins': y_test})\n",
    "ct = pd.crosstab(df1['labels'],df1['wins'])\n",
    "print('kmeans with PCA & normalizer')\n",
    "print(ct)\n",
    "print(\"Score:\", npipeline.score(X_test, y_test))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# F1 = 2 * (precision * recall) / (precision + recall)\n",
    "# Using weighted average to account for label imbalance\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "kmeans.fit(X_train, y_train)\n",
    "labels = kmeans.predict(X_test)\n",
    "print('kmeans')\n",
    "print('f1 score: '+ str(f1_score(y_test, labels, average='weighted')))\n",
    "print('accuracy: '+ str(accuracy_score(y_test, labels)))\n",
    "print()\n",
    "\n",
    "kpipeline.fit(X_train, y_train)\n",
    "labels = kpipeline.predict(X_test)\n",
    "print('kmeans with PCA')\n",
    "print('f1 score: '+ str(f1_score(y_test, labels, average='weighted')))\n",
    "print('accuracy: '+ str(accuracy_score(y_test, labels)))\n",
    "print()\n",
    "\n",
    "spipeline.fit(X_train, y_train)\n",
    "labels = spipeline.predict(X_test)\n",
    "print('kmeans with PCA & scaler')\n",
    "print('f1 score: '+ str(f1_score(y_test, labels, average='weighted')))\n",
    "print('accuracy: '+ str(accuracy_score(y_test, labels)))\n",
    "print()\n",
    "\n",
    "npipeline.fit(X_train, y_train)\n",
    "labels = npipeline.predict(X_test)\n",
    "print('kmeans with PCA & normalizer')\n",
    "print('f1 score: '+ str(f1_score(y_test, labels, average='weighted')))\n",
    "print('accuracy: '+ str(accuracy_score(y_test, labels)))\n",
    "print()\n",
    "\n",
    "'''\n",
    "#ngmpipeline.fit(X_train, y_train)\n",
    "labels = ngmpipeline.fit_predict(X_test)\n",
    "print('hmm with PCA & normalizer')\n",
    "print('f1 score: '+ str(f1_score(y_test, labels, average='weighted')))\n",
    "print('accuracy: '+ str(accuracy_score(y_test, labels)))\n",
    "print()\n",
    "'''\n",
    "\n",
    "print('Hence shown that hmm with PCA & normalizer performs the worst')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then proceed to the supervised learning section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "rfc = RandomForestClassifier()\n",
    "rfc.fit(X_train, y_train)\n",
    "\n",
    "rfc_pred = rfc.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\", rfc.score(X_test, y_test))\n",
    "\n",
    "t2 = time.time()\n",
    "print(\"Time taken:\", t2-t1, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "t1 = time.time()\n",
    "etc = ExtraTreesClassifier()\n",
    "etc.fit(X_train, y_train)\n",
    "\n",
    "etc_pred = etc.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\", etc.score(X_test, y_test))\n",
    "t2 = time.time()\n",
    "print(\"Time taken:\", t2-t1, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "print(\"WARNING! This may take 10 minutes or so on Kaggle (2020)\")\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "for iters in range(10,201,10):\n",
    "    lr = LogisticRegression(max_iter=iters)\n",
    "    lr.fit(X_train, y_train)\n",
    "\n",
    "    lr_pred = lr.predict(X_test)\n",
    "\n",
    "    print(\"Number of iterations:\",  iters, \"of 200\")\n",
    "    #print(lr.score(X_test, y_test))\n",
    "\n",
    "    t2 = time.time()\n",
    "    print(\"Time taken:\", t2-t1, \"seconds\")\n",
    "print(lr.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Random Forest Classifier\")\n",
    "print(rfc.predict_proba(X_test)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Extra Trees Classifier\")\n",
    "print(etc.predict_proba(X_test)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Logistic Regression\")\n",
    "print(lr.predict_proba(X_test)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = {'RandomForestClassifier' : rfc_pred, 'ExtraTreesClassifier': etc_pred, 'LogisticRegression': lr_pred,\n",
    "        'Actual': y_test}\n",
    "res_df = pd.DataFrame(pred, columns = ['RandomForestClassifier', 'ExtraTreesClassifier', 'LogisticRegression',\n",
    "                                       'Actual'])\n",
    "print(res_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "print(\"Outcome Legend\")\n",
    "print(\"0: Away win\")\n",
    "print(\"1: Draw\")\n",
    "print(\"2: Home win\")\n",
    "print()\n",
    "print(\"Confusion matrix for Random Forest Classifier\")\n",
    "print(confusion_matrix(y_test, res_df['RandomForestClassifier']))\n",
    "print(\"Confusion matrix for Extra Trees Classifier\")\n",
    "print(confusion_matrix(y_test, res_df['ExtraTreesClassifier']))\n",
    "print(\"Confusion matrix for Logistic Regression\")\n",
    "print(confusion_matrix(y_test, res_df['LogisticRegression']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us combine the models and check the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "t1 = time.time()\n",
    "eclf1 = VotingClassifier(estimators=[('1', rfc), ('2', etc), ('3', lr)], voting='hard')\n",
    "eclf1.fit(X_train, y_train)\n",
    "print(\"Accuracy:\", eclf1.score(X_test, y_test))\n",
    "t2 = time.time()\n",
    "print(\"Time taken:\", t2-t1, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "t1 = time.time()\n",
    "eclf2 = VotingClassifier(estimators=[('1', rfc), ('2', etc), ('3', lr)], voting='soft')\n",
    "eclf2.fit(X_train, y_train)\n",
    "print(\"Accuracy:\", eclf2.score(X_test, y_test))\n",
    "t2 = time.time()\n",
    "print(\"Time taken:\", t2-t1, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Confusion matrix for Voting Classifier (hard voting)\")\n",
    "print(confusion_matrix(y_test, eclf1.predict(X_test)))\n",
    "print(\"Confusion matrix for Voting Classifier (soft voting)\")\n",
    "print(confusion_matrix(y_test, eclf2.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also tried the TensorFlow machine learning framework, but it failed terribly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(X_train.shape)\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(X_train.shape[1],)),\n",
    "  tf.keras.layers.Dense(128, activation='tanh'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(3)\n",
    "])\n",
    "\n",
    "X_train_numpy = np.asarray(X_train)\n",
    "y_train_numpy = np.asarray(y_train)\n",
    "X_test_numpy = np.asarray(X_test)\n",
    "y_test_numpy = np.asarray(y_test)\n",
    "\n",
    "predictions = model(np.ndarray.astype(X_train_numpy[:1], np.float32)).numpy()\n",
    "\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "model.compile(optimizer='adam',\n",
    "              loss=loss_fn,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "pre_training_result = model.evaluate(np.ndarray.astype(X_test_numpy, np.float32), \n",
    "                                     np.ndarray.astype(y_test_numpy, np.float32), verbose=2)\n",
    "print(\"Pre-training loss:\", pre_training_result[0])\n",
    "print(\"Pre-training accuracy:\", pre_training_result[1])\n",
    "\n",
    "model.fit(np.ndarray.astype(X_train_numpy, np.float32),\n",
    "          np.ndarray.astype(y_train_numpy, np.float32), epochs=10, validation_split=0.2, batch_size=32)\n",
    "\n",
    "probability_model = tf.keras.Sequential([model, tf.keras.layers.Softmax()])\n",
    "\n",
    "predictions = model(np.ndarray.astype(X_test_numpy, np.float32)).numpy()\n",
    "forecast = np.argmax(predictions, axis=1)\n",
    "\n",
    "post_training_result = model.evaluate(np.ndarray.astype(X_test_numpy, np.float32), \n",
    "                                      np.ndarray.astype(y_test_numpy, np.float32), verbose=2)\n",
    "print(\"Post-training loss:\", post_training_result[0])\n",
    "print(\"Post-training accuracy:\", post_training_result[1])\n",
    "print()\n",
    "\n",
    "print(\"Probability prediction\")\n",
    "print(tf.nn.softmax(predictions).numpy()[:10])\n",
    "print(\"Result prediction\")\n",
    "print(forecast[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Confusion matrix: \")\n",
    "print(confusion_matrix(y_test, forecast))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
