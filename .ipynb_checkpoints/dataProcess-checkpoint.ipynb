{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading the data from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data\\database.sqlite\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-1-437a70de9483>:20: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n",
      "  pd.set_option('display.max_colwidth', -1)\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('./data'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Any results you write to the current directory are saved as output.\n",
    "pd.set_option('display.max_rows', 40)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', -1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now move on to the first round of data processing, where values which the full time result does not exist are removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial dataframe shape: (179807, 172)\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import sklearn, numpy\n",
    "import category_encoders as ce\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "con = sqlite3.connect('./data/database.sqlite')\n",
    "\n",
    "df = pd.read_sql_query(\"select * from football_data;\", con)\n",
    "#df = df.sample(frac=0.01)\n",
    "\n",
    "print(\"Initial dataframe shape:\", df.shape)\n",
    "\n",
    "index_names = df[(df['FTR'] != 'H') & (df['FTR'] != 'A') & (df['FTR'] != 'D')].index\n",
    "\n",
    "#df = df.sort_values('Datetime')\n",
    "\n",
    "df.drop(index_names, inplace=True)\n",
    "\n",
    "column_names = list(df.columns.values)\n",
    "#df.dropna(inplace=True, thresh=int(0.8*df.shape[0]) , axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Country' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-800675de1c5d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mHendDa\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mCountry\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;34m'China'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'Country' is not defined"
     ]
    }
   ],
   "source": [
    "HendDa = df[HOm]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will then do data preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(179803, 140)\n"
     ]
    }
   ],
   "source": [
    "# The columns that contain significant number of null values\n",
    "noneSet = []\n",
    "noneSet = ['ABP', 'AFKC', 'AHW', 'AO', 'AT', 'Attendance', 'BSA', 'BSD', 'BSH', \n",
    "           'Bb1X2', 'BbAH', 'BbAHh', 'BbAv<2.5', 'BbAv>2.5', 'BbAvA', 'BbAvAHA', 'BbAvAHH', 'BbAvD', \n",
    "           'BbAvH', 'BbMx<2.5', 'BbMx>2.5', 'B365AH', 'BbMxA', 'BbMxAHA', 'BbMxAHH', 'BbMxD', 'BbMxH', \n",
    "           'BbOU', 'GB<2.5', 'GB>2.5', 'GBA', 'GBAH', 'GBAHA', 'GBAHH', 'GBD', 'GBH', 'HBP', 'HFKC', 'HHW', \n",
    "           'HO', 'HT', 'LBAH', 'LBAHA', 'LBAHH', 'SBA', 'SBD', 'SBH', 'SJA', 'SJD', 'SJH', 'SOA', 'SOD', 'SOH',\n",
    "           'SYA', 'SYD', 'SYH', 'PA', 'PD', 'PH', 'LBH', 'LBD', 'LBA']\n",
    "\n",
    "allow_halftime = False\n",
    "\n",
    "# The value of these columns is not known before the match\n",
    "if not allow_halftime:\n",
    "    results = ['FTHG', 'FTAG', 'FTR', 'HTHG', 'HTAG', 'HTR']\n",
    "else:\n",
    "    # To allow half time statistics, use this\n",
    "    results = ['FTHG', 'FTAG', 'FTR', 'HTR']\n",
    "\n",
    "# The value of these columns are not known before the match as well\n",
    "match_statistics = ['Attendance', 'Referee', 'HS', 'AS', 'HST', 'AST', 'HHW', 'AHW', 'HC', 'AC', 'HF', 'AF', 'HFKC', \n",
    " 'AFKC', 'HO', 'AO', 'HY', 'AY', 'HR', 'AR', 'HBP', 'ABP']\n",
    "\n",
    "X = df.drop(set(results + match_statistics + ['Datetime', 'Season', 'HT', \n",
    "                                              'AT']).intersection(column_names), axis=1)\n",
    "print(X.shape)\n",
    "\n",
    "# Desired result\n",
    "y = df['FTR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final dataframe shape: (179803, 172)\n",
      "      Season             Datetime  Div   Country          League   Referee  \\\n",
      "0  2020       2020-08-21 00:30:00  USA  USA       MLS             -1         \n",
      "1  2020       2020-08-21 00:30:00  USA  USA       MLS             -1         \n",
      "2  2020       2020-08-21 00:00:00  BRA  Brazil    Serie A         -1         \n",
      "3  2020       2020-08-21 00:00:00  USA  USA       MLS             -1         \n",
      "4  2020       2020-08-21 00:00:00  BRA  Brazil    Serie A         -1         \n",
      "5  2020       2020-08-20 23:15:00  BRA  Brazil    Serie A         -1         \n",
      "6  2020/2021  2020-08-20 19:30:00  SC0  Scotland  Premier League  W Collum   \n",
      "7  2020       2020-08-20 13:00:00  CHN  China     Super League    -1         \n",
      "8  2020       2020-08-20 11:00:00  CHN  China     Super League    -1         \n",
      "9  2020       2020-08-20 01:30:00  BRA  Brazil    Serie A         -1         \n",
      "\n",
      "                 HomeTeam            AwayTeam  FTHG  FTAG FTR  HTHG  HTAG HTR  \\\n",
      "0  Columbus Crew           Chicago Fire        3     0     H  -1    -1     -1   \n",
      "1  New England Revolution  Philadelphia Union  0     0     D  -1    -1     -1   \n",
      "2  Ceara                   Vasco               0     3     A  -1    -1     -1   \n",
      "3  New York Red Bulls      New York City       1     0     H  -1    -1     -1   \n",
      "4  Sao Paulo               Bahia               1     1     D  -1    -1     -1   \n",
      "5  Sport Recife            Santos              0     1     A  -1    -1     -1   \n",
      "6  St Johnstone            Aberdeen            0     1     A   0     0     D    \n",
      "7  Shenzhen                Dalian Pro          3     2     H  -1    -1     -1   \n",
      "8  Guangzhou Evergrande    Jiangsu Suning      2     1     H  -1    -1     -1   \n",
      "9  Corinthians             Coritiba            3     1     H  -1    -1     -1   \n",
      "\n",
      "        PSH       PSD       PSA     B365H     B365D     B365A       LBH  \\\n",
      "0  0.350877  0.350877  0.350877  0.350877  0.350877  0.350877  0.350877   \n",
      "1  0.350877  0.350877  0.350877  0.350877  0.350877  0.350877  0.350877   \n",
      "2  0.350877  0.350877  0.350877  0.350877  0.350877  0.350877  0.350877   \n",
      "3  0.350877  0.350877  0.350877  0.350877  0.350877  0.350877  0.350877   \n",
      "4  0.350877  0.350877  0.350877  0.350877  0.350877  0.350877  0.350877   \n",
      "5  0.350877  0.350877  0.350877  0.350877  0.350877  0.350877  0.350877   \n",
      "6  0.350877  0.350877  0.350877  0.392157  0.322581  0.348432  0.350877   \n",
      "7  0.350877  0.350877  0.350877  0.350877  0.350877  0.350877  0.350877   \n",
      "8  0.350877  0.350877  0.350877  0.350877  0.350877  0.350877  0.350877   \n",
      "9  0.350877  0.350877  0.350877  0.350877  0.350877  0.350877  0.350877   \n",
      "\n",
      "        LBD       LBA       BWH       BWD       BWA  ABP   AC    AF  AFKC  \\\n",
      "0  0.350877  0.350877  0.350877  0.350877  0.350877 -1.0 -1.0 -1.0  -1.0    \n",
      "1  0.350877  0.350877  0.350877  0.350877  0.350877 -1.0 -1.0 -1.0  -1.0    \n",
      "2  0.350877  0.350877  0.350877  0.350877  0.350877 -1.0 -1.0 -1.0  -1.0    \n",
      "3  0.350877  0.350877  0.350877  0.350877  0.350877 -1.0 -1.0 -1.0  -1.0    \n",
      "4  0.350877  0.350877  0.350877  0.350877  0.350877 -1.0 -1.0 -1.0  -1.0    \n",
      "5  0.350877  0.350877  0.350877  0.350877  0.350877 -1.0 -1.0 -1.0  -1.0    \n",
      "6  0.350877  0.350877  0.384615  0.322581  0.363636 -1.0  1.0  13.0 -1.0    \n",
      "7  0.350877  0.350877  0.350877  0.350877  0.350877 -1.0 -1.0 -1.0  -1.0    \n",
      "8  0.350877  0.350877  0.350877  0.350877  0.350877 -1.0 -1.0 -1.0  -1.0    \n",
      "9  0.350877  0.350877  0.350877  0.350877  0.350877 -1.0 -1.0 -1.0  -1.0    \n",
      "\n",
      "   AHCh  AHW       AHh   AO   AR   AS  AST  AT   AY  Attendance   Avg<2.5  \\\n",
      "0 -1.0  -1.0  0.526316 -1.0 -1.0 -1.0 -1.0  -1 -1.0 -1.0         0.526316   \n",
      "1 -1.0  -1.0  0.526316 -1.0 -1.0 -1.0 -1.0  -1 -1.0 -1.0         0.526316   \n",
      "2 -1.0  -1.0  0.526316 -1.0 -1.0 -1.0 -1.0  -1 -1.0 -1.0         0.526316   \n",
      "3 -1.0  -1.0  0.526316 -1.0 -1.0 -1.0 -1.0  -1 -1.0 -1.0         0.526316   \n",
      "4 -1.0  -1.0  0.526316 -1.0 -1.0 -1.0 -1.0  -1 -1.0 -1.0         0.526316   \n",
      "5 -1.0  -1.0  0.526316 -1.0 -1.0 -1.0 -1.0  -1 -1.0 -1.0         0.526316   \n",
      "6  0.0  -1.0  0.000000 -1.0  0.0  6.0  3.0  -1  3.0 -1.0         0.671141   \n",
      "7 -1.0  -1.0  0.526316 -1.0 -1.0 -1.0 -1.0  -1 -1.0 -1.0         0.526316   \n",
      "8 -1.0  -1.0  0.526316 -1.0 -1.0 -1.0 -1.0  -1 -1.0 -1.0         0.526316   \n",
      "9 -1.0  -1.0  0.526316 -1.0 -1.0 -1.0 -1.0  -1 -1.0 -1.0         0.526316   \n",
      "\n",
      "    Avg>2.5      AvgA    AvgAHA    AvgAHH  AvgC<2.5  AvgC>2.5     AvgCA  \\\n",
      "0  0.526316  0.230415  0.526316  0.526316  0.526316  0.526316  0.350877   \n",
      "1  0.526316  0.325733  0.526316  0.526316  0.526316  0.526316  0.350877   \n",
      "2  0.526316  0.255102  0.526316  0.526316  0.526316  0.526316  0.350877   \n",
      "3  0.526316  0.401606  0.526316  0.526316  0.526316  0.526316  0.350877   \n",
      "4  0.526316  0.194932  0.526316  0.526316  0.526316  0.526316  0.350877   \n",
      "5  0.526316  0.341297  0.526316  0.526316  0.526316  0.526316  0.350877   \n",
      "6  0.387597  0.353357  0.497512  0.543478  0.689655  0.364964  0.378788   \n",
      "7  0.526316  0.469484  0.526316  0.526316  0.526316  0.526316  0.350877   \n",
      "8  0.526316  0.276243  0.526316  0.526316  0.526316  0.526316  0.350877   \n",
      "9  0.526316  0.200000  0.526316  0.526316  0.526316  0.526316  0.350877   \n",
      "\n",
      "    AvgCAHA   AvgCAHH     AvgCD     AvgCH      AvgD      AvgH  B365<2.5  \\\n",
      "0  0.526316  0.526316  0.350877  0.350877  0.276243  0.552486  0.526316   \n",
      "1  0.526316  0.526316  0.350877  0.350877  0.276243  0.458716  0.526316   \n",
      "2  0.526316  0.526316  0.350877  0.350877  0.325733  0.483092  0.526316   \n",
      "3  0.526316  0.526316  0.350877  0.350877  0.283286  0.377358  0.526316   \n",
      "4  0.526316  0.526316  0.350877  0.350877  0.293255  0.574713  0.526316   \n",
      "5  0.526316  0.526316  0.350877  0.350877  0.317460  0.408163  0.526316   \n",
      "6  0.546448  0.495050  0.338983  0.346021  0.325733  0.387597  0.653595   \n",
      "7  0.526316  0.526316  0.350877  0.350877  0.286533  0.319489  0.526316   \n",
      "8  0.526316  0.526316  0.350877  0.350877  0.295858  0.497512  0.526316   \n",
      "9  0.526316  0.526316  0.350877  0.350877  0.318471  0.546448  0.526316   \n",
      "\n",
      "   B365>2.5    B365AH   B365AHA   B365AHH  B365C<2.5  B365C>2.5    B365CA  \\\n",
      "0  0.526316  0.526316  0.526316  0.526316  0.526316   0.526316   0.350877   \n",
      "1  0.526316  0.526316  0.526316  0.526316  0.526316   0.526316   0.350877   \n",
      "2  0.526316  0.526316  0.526316  0.526316  0.526316   0.526316   0.350877   \n",
      "3  0.526316  0.526316  0.526316  0.526316  0.526316   0.526316   0.350877   \n",
      "4  0.526316  0.526316  0.526316  0.526316  0.526316   0.526316   0.350877   \n",
      "5  0.526316  0.526316  0.526316  0.526316  0.526316   0.526316   0.350877   \n",
      "6  0.400000  0.526316  0.526316  0.526316  0.714286   0.333333   0.392157   \n",
      "7  0.526316  0.526316  0.526316  0.526316  0.526316   0.526316   0.350877   \n",
      "8  0.526316  0.526316  0.526316  0.526316  0.526316   0.526316   0.350877   \n",
      "9  0.526316  0.526316  0.526316  0.526316  0.526316   0.526316   0.350877   \n",
      "\n",
      "   B365CAHA  B365CAHH    B365CD    B365CH       BSA       BSD       BSH  \\\n",
      "0  0.526316  0.526316  0.350877  0.350877  0.350877  0.350877  0.350877   \n",
      "1  0.526316  0.526316  0.350877  0.350877  0.350877  0.350877  0.350877   \n",
      "2  0.526316  0.526316  0.350877  0.350877  0.350877  0.350877  0.350877   \n",
      "3  0.526316  0.526316  0.350877  0.350877  0.350877  0.350877  0.350877   \n",
      "4  0.526316  0.526316  0.350877  0.350877  0.350877  0.350877  0.350877   \n",
      "5  0.526316  0.526316  0.350877  0.350877  0.350877  0.350877  0.350877   \n",
      "6  0.555556  0.487805  0.322581  0.348432  0.350877  0.350877  0.350877   \n",
      "7  0.526316  0.526316  0.350877  0.350877  0.350877  0.350877  0.350877   \n",
      "8  0.526316  0.526316  0.350877  0.350877  0.350877  0.350877  0.350877   \n",
      "9  0.526316  0.526316  0.350877  0.350877  0.350877  0.350877  0.350877   \n",
      "\n",
      "       BWCA      BWCD      BWCH     Bb1X2 BbAH  BbAHh  BbAv<2.5  BbAv>2.5  \\\n",
      "0  0.350877  0.350877  0.350877  0.350877  -1  -1.0    0.526316  0.526316   \n",
      "1  0.350877  0.350877  0.350877  0.350877  -1  -1.0    0.526316  0.526316   \n",
      "2  0.350877  0.350877  0.350877  0.350877  -1  -1.0    0.526316  0.526316   \n",
      "3  0.350877  0.350877  0.350877  0.350877  -1  -1.0    0.526316  0.526316   \n",
      "4  0.350877  0.350877  0.350877  0.350877  -1  -1.0    0.526316  0.526316   \n",
      "5  0.350877  0.350877  0.350877  0.350877  -1  -1.0    0.526316  0.526316   \n",
      "6  0.370370  0.322581  0.377358  0.350877  -1  -1.0    0.526316  0.526316   \n",
      "7  0.350877  0.350877  0.350877  0.350877  -1  -1.0    0.526316  0.526316   \n",
      "8  0.350877  0.350877  0.350877  0.350877  -1  -1.0    0.526316  0.526316   \n",
      "9  0.350877  0.350877  0.350877  0.350877  -1  -1.0    0.526316  0.526316   \n",
      "\n",
      "      BbAvA   BbAvAHA   BbAvAHH     BbAvD     BbAvH  BbMx<2.5  BbMx>2.5  \\\n",
      "0  0.350877  0.526316  0.526316  0.350877  0.350877  0.526316  0.526316   \n",
      "1  0.350877  0.526316  0.526316  0.350877  0.350877  0.526316  0.526316   \n",
      "2  0.350877  0.526316  0.526316  0.350877  0.350877  0.526316  0.526316   \n",
      "3  0.350877  0.526316  0.526316  0.350877  0.350877  0.526316  0.526316   \n",
      "4  0.350877  0.526316  0.526316  0.350877  0.350877  0.526316  0.526316   \n",
      "5  0.350877  0.526316  0.526316  0.350877  0.350877  0.526316  0.526316   \n",
      "6  0.350877  0.526316  0.526316  0.350877  0.350877  0.526316  0.526316   \n",
      "7  0.350877  0.526316  0.526316  0.350877  0.350877  0.526316  0.526316   \n",
      "8  0.350877  0.526316  0.526316  0.350877  0.350877  0.526316  0.526316   \n",
      "9  0.350877  0.526316  0.526316  0.350877  0.350877  0.526316  0.526316   \n",
      "\n",
      "      BbMxA   BbMxAHA   BbMxAHH     BbMxD     BbMxH  BbOU   Date    GB<2.5  \\\n",
      "0  0.350877  0.526316  0.526316  0.350877  0.350877 -1.0   18495  0.526316   \n",
      "1  0.350877  0.526316  0.526316  0.350877  0.350877 -1.0   18495  0.526316   \n",
      "2  0.350877  0.526316  0.526316  0.350877  0.350877 -1.0   18495  0.526316   \n",
      "3  0.350877  0.526316  0.526316  0.350877  0.350877 -1.0   18495  0.526316   \n",
      "4  0.350877  0.526316  0.526316  0.350877  0.350877 -1.0   18495  0.526316   \n",
      "5  0.350877  0.526316  0.526316  0.350877  0.350877 -1.0   18494  0.526316   \n",
      "6  0.350877  0.526316  0.526316  0.350877  0.350877 -1.0   18494  0.526316   \n",
      "7  0.350877  0.526316  0.526316  0.350877  0.350877 -1.0   18494  0.526316   \n",
      "8  0.350877  0.526316  0.526316  0.350877  0.350877 -1.0   18494  0.526316   \n",
      "9  0.350877  0.526316  0.526316  0.350877  0.350877 -1.0   18494  0.526316   \n",
      "\n",
      "     GB>2.5       GBA      GBAH     GBAHA     GBAHH       GBD       GBH  HBP  \\\n",
      "0  0.526316  0.350877  0.526316  0.526316  0.526316  0.350877  0.350877 -1.0   \n",
      "1  0.526316  0.350877  0.526316  0.526316  0.526316  0.350877  0.350877 -1.0   \n",
      "2  0.526316  0.350877  0.526316  0.526316  0.526316  0.350877  0.350877 -1.0   \n",
      "3  0.526316  0.350877  0.526316  0.526316  0.526316  0.350877  0.350877 -1.0   \n",
      "4  0.526316  0.350877  0.526316  0.526316  0.526316  0.350877  0.350877 -1.0   \n",
      "5  0.526316  0.350877  0.526316  0.526316  0.526316  0.350877  0.350877 -1.0   \n",
      "6  0.526316  0.350877  0.526316  0.526316  0.526316  0.350877  0.350877 -1.0   \n",
      "7  0.526316  0.350877  0.526316  0.526316  0.526316  0.350877  0.350877 -1.0   \n",
      "8  0.526316  0.350877  0.526316  0.526316  0.526316  0.350877  0.350877 -1.0   \n",
      "9  0.526316  0.350877  0.526316  0.526316  0.526316  0.350877  0.350877 -1.0   \n",
      "\n",
      "    HC    HF  HFKC  HHW   HO   HR    HS  HST  HT   HY       IWA      IWCA  \\\n",
      "0 -1.0 -1.0  -1.0  -1.0 -1.0 -1.0 -1.0  -1.0  -1 -1.0  0.350877  0.350877   \n",
      "1 -1.0 -1.0  -1.0  -1.0 -1.0 -1.0 -1.0  -1.0  -1 -1.0  0.350877  0.350877   \n",
      "2 -1.0 -1.0  -1.0  -1.0 -1.0 -1.0 -1.0  -1.0  -1 -1.0  0.350877  0.350877   \n",
      "3 -1.0 -1.0  -1.0  -1.0 -1.0 -1.0 -1.0  -1.0  -1 -1.0  0.350877  0.350877   \n",
      "4 -1.0 -1.0  -1.0  -1.0 -1.0 -1.0 -1.0  -1.0  -1 -1.0  0.350877  0.350877   \n",
      "5 -1.0 -1.0  -1.0  -1.0 -1.0 -1.0 -1.0  -1.0  -1 -1.0  0.350877  0.350877   \n",
      "6  5.0  17.0 -1.0  -1.0 -1.0  0.0  11.0  1.0  -1  1.0  0.350877  0.377358   \n",
      "7 -1.0 -1.0  -1.0  -1.0 -1.0 -1.0 -1.0  -1.0  -1 -1.0  0.350877  0.350877   \n",
      "8 -1.0 -1.0  -1.0  -1.0 -1.0 -1.0 -1.0  -1.0  -1 -1.0  0.350877  0.350877   \n",
      "9 -1.0 -1.0  -1.0  -1.0 -1.0 -1.0 -1.0  -1.0  -1 -1.0  0.350877  0.350877   \n",
      "\n",
      "       IWCD      IWCH       IWD       IWH      LBAH     LBAHA     LBAHH  \\\n",
      "0  0.350877  0.350877  0.350877  0.350877  0.526316  0.526316  0.526316   \n",
      "1  0.350877  0.350877  0.350877  0.350877  0.526316  0.526316  0.526316   \n",
      "2  0.350877  0.350877  0.350877  0.350877  0.526316  0.526316  0.526316   \n",
      "3  0.350877  0.350877  0.350877  0.350877  0.526316  0.526316  0.526316   \n",
      "4  0.350877  0.350877  0.350877  0.350877  0.526316  0.526316  0.526316   \n",
      "5  0.350877  0.350877  0.350877  0.350877  0.526316  0.526316  0.526316   \n",
      "6  0.357143  0.344828  0.338983  0.384615  0.526316  0.526316  0.526316   \n",
      "7  0.350877  0.350877  0.350877  0.350877  0.526316  0.526316  0.526316   \n",
      "8  0.350877  0.350877  0.350877  0.350877  0.526316  0.526316  0.526316   \n",
      "9  0.350877  0.350877  0.350877  0.350877  0.526316  0.526316  0.526316   \n",
      "\n",
      "    Max<2.5   Max>2.5      MaxA    MaxAHA    MaxAHH  MaxC<2.5  MaxC>2.5  \\\n",
      "0  0.526316  0.526316  0.215054  0.526316  0.526316  0.526316  0.526316   \n",
      "1  0.526316  0.526316  0.311526  0.526316  0.526316  0.526316  0.526316   \n",
      "2  0.526316  0.526316  0.232558  0.526316  0.526316  0.526316  0.526316   \n",
      "3  0.526316  0.526316  0.384615  0.526316  0.526316  0.526316  0.526316   \n",
      "4  0.526316  0.526316  0.169492  0.526316  0.526316  0.526316  0.526316   \n",
      "5  0.526316  0.526316  0.319489  0.526316  0.526316  0.526316  0.526316   \n",
      "6  0.641026  0.361011  0.328947  0.478469  0.523560  0.657895  0.333333   \n",
      "7  0.526316  0.526316  0.440529  0.526316  0.526316  0.526316  0.526316   \n",
      "8  0.526316  0.526316  0.238095  0.526316  0.526316  0.526316  0.526316   \n",
      "9  0.526316  0.526316  0.176991  0.526316  0.526316  0.526316  0.526316   \n",
      "\n",
      "      MaxCA   MaxCAHA   MaxCAHH     MaxCD     MaxCH      MaxD      MaxH  \\\n",
      "0  0.350877  0.526316  0.526316  0.350877  0.350877  0.263158  0.529101   \n",
      "1  0.350877  0.526316  0.526316  0.350877  0.350877  0.266667  0.434783   \n",
      "2  0.350877  0.526316  0.526316  0.350877  0.350877  0.297619  0.465116   \n",
      "3  0.350877  0.526316  0.526316  0.350877  0.350877  0.266667  0.344828   \n",
      "4  0.350877  0.526316  0.526316  0.350877  0.350877  0.274725  0.534759   \n",
      "5  0.350877  0.526316  0.526316  0.350877  0.350877  0.289017  0.377358   \n",
      "6  0.361011  0.520833  0.473934  0.316456  0.324675  0.308642  0.363636   \n",
      "7  0.350877  0.526316  0.526316  0.350877  0.350877  0.273224  0.284091   \n",
      "8  0.350877  0.526316  0.526316  0.350877  0.350877  0.268817  0.458716   \n",
      "9  0.350877  0.526316  0.526316  0.350877  0.350877  0.285714  0.520833   \n",
      "\n",
      "      P<2.5     P>2.5        PA      PAHA      PAHH    PC<2.5    PC>2.5  \\\n",
      "0  0.526316  0.526316  0.226244  0.526316  0.526316  0.526316  0.526316   \n",
      "1  0.526316  0.526316  0.317460  0.526316  0.526316  0.526316  0.526316   \n",
      "2  0.526316  0.526316  0.245098  0.526316  0.526316  0.526316  0.526316   \n",
      "3  0.526316  0.526316  0.392157  0.526316  0.526316  0.526316  0.526316   \n",
      "4  0.526316  0.526316  0.191571  0.526316  0.526316  0.526316  0.526316   \n",
      "5  0.526316  0.526316  0.335570  0.526316  0.526316  0.526316  0.526316   \n",
      "6  0.526316  0.526316  0.350877  0.526316  0.526316  0.699301  0.341297   \n",
      "7  0.526316  0.526316  0.446429  0.526316  0.526316  0.526316  0.526316   \n",
      "8  0.526316  0.526316  0.275482  0.526316  0.526316  0.526316  0.526316   \n",
      "9  0.526316  0.526316  0.195312  0.526316  0.526316  0.526316  0.526316   \n",
      "\n",
      "      PCAHA     PCAHH        PD        PH      PSCA      PSCD      PSCH  \\\n",
      "0  0.526316  0.526316  0.277008  0.534759  0.350877  0.350877  0.350877   \n",
      "1  0.526316  0.526316  0.268817  0.452489  0.350877  0.350877  0.350877   \n",
      "2  0.526316  0.526316  0.317460  0.483092  0.350877  0.350877  0.350877   \n",
      "3  0.526316  0.526316  0.274725  0.371747  0.350877  0.350877  0.350877   \n",
      "4  0.526316  0.526316  0.279330  0.574713  0.350877  0.350877  0.350877   \n",
      "5  0.526316  0.526316  0.306748  0.403226  0.350877  0.350877  0.350877   \n",
      "6  0.540541  0.487805  0.350877  0.350877  0.369004  0.336700  0.334448   \n",
      "7  0.526316  0.526316  0.274725  0.324675  0.350877  0.350877  0.350877   \n",
      "8  0.526316  0.526316  0.294985  0.476190  0.350877  0.350877  0.350877   \n",
      "9  0.526316  0.526316  0.315457  0.534759  0.350877  0.350877  0.350877   \n",
      "\n",
      "        SBA       SBD       SBH       SJA       SJD       SJH       SOA  \\\n",
      "0  0.350877  0.350877  0.350877  0.350877  0.350877  0.350877  0.350877   \n",
      "1  0.350877  0.350877  0.350877  0.350877  0.350877  0.350877  0.350877   \n",
      "2  0.350877  0.350877  0.350877  0.350877  0.350877  0.350877  0.350877   \n",
      "3  0.350877  0.350877  0.350877  0.350877  0.350877  0.350877  0.350877   \n",
      "4  0.350877  0.350877  0.350877  0.350877  0.350877  0.350877  0.350877   \n",
      "5  0.350877  0.350877  0.350877  0.350877  0.350877  0.350877  0.350877   \n",
      "6  0.350877  0.350877  0.350877  0.350877  0.350877  0.350877  0.350877   \n",
      "7  0.350877  0.350877  0.350877  0.350877  0.350877  0.350877  0.350877   \n",
      "8  0.350877  0.350877  0.350877  0.350877  0.350877  0.350877  0.350877   \n",
      "9  0.350877  0.350877  0.350877  0.350877  0.350877  0.350877  0.350877   \n",
      "\n",
      "        SOD       SOH       SYA       SYD       SYH  Time       VCA      VCCA  \\\n",
      "0  0.350877  0.350877  0.350877  0.350877  0.350877  30    0.350877  0.350877   \n",
      "1  0.350877  0.350877  0.350877  0.350877  0.350877  30    0.350877  0.350877   \n",
      "2  0.350877  0.350877  0.350877  0.350877  0.350877  0     0.350877  0.350877   \n",
      "3  0.350877  0.350877  0.350877  0.350877  0.350877  0     0.350877  0.350877   \n",
      "4  0.350877  0.350877  0.350877  0.350877  0.350877  0     0.350877  0.350877   \n",
      "5  0.350877  0.350877  0.350877  0.350877  0.350877  1395  0.350877  0.350877   \n",
      "6  0.350877  0.350877  0.350877  0.350877  0.350877  1170  0.347222  0.384615   \n",
      "7  0.350877  0.350877  0.350877  0.350877  0.350877  780   0.350877  0.350877   \n",
      "8  0.350877  0.350877  0.350877  0.350877  0.350877  660   0.350877  0.350877   \n",
      "9  0.350877  0.350877  0.350877  0.350877  0.350877  90    0.350877  0.350877   \n",
      "\n",
      "       VCCD      VCCH       VCD       VCH       WHA      WHCA      WHCD  \\\n",
      "0  0.350877  0.350877  0.350877  0.350877  0.350877  0.350877  0.350877   \n",
      "1  0.350877  0.350877  0.350877  0.350877  0.350877  0.350877  0.350877   \n",
      "2  0.350877  0.350877  0.350877  0.350877  0.350877  0.350877  0.350877   \n",
      "3  0.350877  0.350877  0.350877  0.350877  0.350877  0.350877  0.350877   \n",
      "4  0.350877  0.350877  0.350877  0.350877  0.350877  0.350877  0.350877   \n",
      "5  0.350877  0.350877  0.350877  0.350877  0.350877  0.350877  0.350877   \n",
      "6  0.333333  0.333333  0.322581  0.384615  0.347222  0.384615  0.333333   \n",
      "7  0.350877  0.350877  0.350877  0.350877  0.350877  0.350877  0.350877   \n",
      "8  0.350877  0.350877  0.350877  0.350877  0.350877  0.350877  0.350877   \n",
      "9  0.350877  0.350877  0.350877  0.350877  0.350877  0.350877  0.350877   \n",
      "\n",
      "       WHCH       WHD       WHH  \n",
      "0  0.350877  0.350877  0.350877  \n",
      "1  0.350877  0.350877  0.350877  \n",
      "2  0.350877  0.350877  0.350877  \n",
      "3  0.350877  0.350877  0.350877  \n",
      "4  0.350877  0.350877  0.350877  \n",
      "5  0.350877  0.350877  0.350877  \n",
      "6  0.344828  0.322581  0.392157  \n",
      "7  0.350877  0.350877  0.350877  \n",
      "8  0.350877  0.350877  0.350877  \n",
      "9  0.350877  0.350877  0.350877  \n"
     ]
    }
   ],
   "source": [
    "import datetime, time\n",
    "\n",
    "# https://stackoverflow.com/questions/57330482/convert-data-frame-datatime-string-to-float-in-python-pandas\n",
    "def convertDate(dateString):\n",
    "    dateTime1 = datetime.datetime.strptime(dateString, '%Y-%m-%d %H:%M:%S')\n",
    "    #ignores time of the day\n",
    "    return int(time.mktime(dateTime1.timetuple()))//86400 \n",
    "\n",
    "def convertTime(timeString):\n",
    "    hour, minute = map(int, timeString.split(':'))\n",
    "    return hour*60+minute\n",
    "\n",
    "def reciprocal(x):\n",
    "    if x == 0:\n",
    "        return 0\n",
    "    return 1/float(x)\n",
    "\n",
    "df['Date'] = df['Date'].apply(convertDate)\n",
    "df['Time'] = df['Time'].apply(convertTime)\n",
    "\n",
    "# Here are a list of betting odds field\n",
    "# Taken from http://www.football-data.co.uk/notes.txt\n",
    "betting_odds = ['B365H', 'B365D', 'B365A', 'BSH', 'BSD', 'BSA', 'BWH', 'BWD', 'BWA', 'GBH', 'GBD', \n",
    "'GBA', 'IWH', 'IWD', 'IWA', 'LBH', 'LBD', 'LBA', 'PH', 'PD', 'PA', 'SOH', 'SOD', 'SOA', 'SBH', 'SBD', 'SBA',\n",
    "'SJH', 'SJD', 'SJA', 'SYH', 'SYD', 'SYA', 'VCH', 'VCD', 'VCA', 'WHH', 'WHD', 'WHA', 'Bb1X2', 'BbMxH', 'BbAvH', \n",
    "'BbMxD', 'BbAvD', 'BbMxA', 'BbAvA', 'MaxH', 'MaxD', 'MaxA', 'AvgH', 'AvgD', 'AvgA', 'B365CH', 'B365CD', 'B365CA', \n",
    "'BWCH', 'BWCD', 'BWCA', 'IWCH', 'IWCD', 'IWCA', 'VCCH', 'VCCD', 'VCCA', 'WHCH', 'WHCD', 'WHCA', 'MaxCH', 'MaxCD', \n",
    "'MaxCA', 'AvgCH', 'AvgCD', 'AvgCA', 'PSH', 'PSD', 'PSA', 'PSCA', 'PSCD', 'PSCH']\n",
    "\n",
    "num_goals_odds = ['BbMx>2.5', 'BbAv>2.5',\n",
    "'BbMx<2.5', 'BbAv<2.5', 'GB>2.5', 'GB<2.5', 'B365>2.5', 'B365<2.5', 'P>2.5', 'P<2.5', 'Max>2.5', 'Max<2.5', \n",
    "'Avg>2.5', 'Avg<2.5', 'B365C>2.5', 'B365C<2.5', 'PC>2.5', 'PC<2.5', 'MaxC>2.5', \n",
    "'MaxC<2.5', 'AvgC>2.5', 'AvgC<2.5']\n",
    "\n",
    "asian_odds = ['AHh', 'BbMxAHH', 'BbAvAHH', 'BbMxAHA', 'BbAvAHA', 'GBAHH', 'GBAHA','GBAH','LBAHH',\n",
    "'LBAHA','LBAH','B365AHH','B365AHA','B365AH','PAHH','PAHA','MaxAHH','MaxAHA','AvgAHH','AvgAHA', \n",
    "'B365CAHH','B365CAHA','PCAHH','PCAHA','MaxCAHH','MaxCAHA','AvgCAHH','AvgCAHA']\n",
    "\n",
    "for i in betting_odds:\n",
    "    df[i].fillna(2.85, inplace=True)\n",
    "\n",
    "for i in num_goals_odds+asian_odds:\n",
    "    df[i].fillna(1.90, inplace=True)\n",
    "\n",
    "column_names = list(df.columns.values)\n",
    "\n",
    "#Replace empty values with -1\n",
    "for i in column_names:\n",
    "    df[i].fillna(-1, inplace=True)\n",
    "\n",
    "\n",
    "for col in betting_odds+num_goals_odds+asian_odds:\n",
    "    try:\n",
    "        df[col] = df[col].apply(reciprocal)\n",
    "    except KeyError:\n",
    "        continue\n",
    "        #print(col, \"KeyError\")\n",
    "    except:\n",
    "        print(col)\n",
    "        raise\n",
    "\n",
    "        \n",
    "def conv_BbAHh(i):\n",
    "    try:\n",
    "        return float(i)\n",
    "    except:\n",
    "        # print(i)\n",
    "        a, b = map(float, i.split(','))\n",
    "        # print(a, b)\n",
    "        return (a+b)/2\n",
    "\n",
    "try:\n",
    "    df['BbAHh'] = df['BbAHh'].apply(conv_BbAHh)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "print(\"Final dataframe shape:\", df.shape)\n",
    "print(df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(179803, 140)\n",
      "0    H\n",
      "1    D\n",
      "2    A\n",
      "3    H\n",
      "4    D\n",
      "5    A\n",
      "6    A\n",
      "7    H\n",
      "8    H\n",
      "9    H\n",
      "Name: FTR, dtype: object\n",
      "[2 1 0 2 1 0 0 2 2 2]\n"
     ]
    }
   ],
   "source": [
    "X = df.drop(set(results + match_statistics + ['Datetime', 'Season', 'HT', \n",
    "                                              'AT']).intersection(column_names), axis=1)\n",
    "print(X.shape)\n",
    "\n",
    "# Desired result\n",
    "y = df['FTR']\n",
    "\n",
    "ce_binaryX = ce.BinaryEncoder(cols = ['HomeTeam', 'AwayTeam', 'Div', 'League', 'Country'])\n",
    "ohe = ce.OneHotEncoder(cols = [])\n",
    "X = ce_binaryX.fit_transform(X)\n",
    "X = ohe.fit_transform(X)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(['H', 'D', 'A'])\n",
    "print(y[:10])\n",
    "y = label_encoder.fit_transform(y)\n",
    "print(y[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "import matplotlib.pyplot as plt\n",
    "columns = list(X.columns)\n",
    "print(columns[0])\n",
    "for i in columns:\n",
    "    plt.scatter(X[i],y)\n",
    "    plt.ylabel('Win')\n",
    "    plt.title('Scatter plot of Win against ' + i)\n",
    "    plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then proceed to the unsupervised learning section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 153\n",
    "\n",
    "# 80-20 split on training and testing data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import SpectralClustering\n",
    "import matplotlib.pyplot as plt\n",
    "from hmmlearn import hmm # Hidden Markovnikov model\n",
    "\n",
    "scaler = StandardScaler()\n",
    "pca = PCA()\n",
    "pca10 = PCA(n_components=10)\n",
    "pipeline = make_pipeline(scaler,pca)\n",
    "pipeline10 = make_pipeline(scaler,pca10)\n",
    "\n",
    "pipeline.fit(X_train)\n",
    "pipeline10.fit(X_train)\n",
    "\n",
    "# Plot the explained variances of the first 10 components\n",
    "features = range(pca10.n_components_)\n",
    "plt.bar(features, pca10.explained_variance_)\n",
    "plt.xlabel('PCA feature')\n",
    "plt.ylabel('variance')\n",
    "plt.xticks(features)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out that from the graph above, only 4 components of the PCA reduction have significant variance. Hence, we will use 4 dimensions in our reduced dimensionality space. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We tried to scale the input and fit the data but to no avail. The clustering turns out to be unable to differentiate between wins, draws and losses. With regards to the score, kmeans with PCA and normaliser is the best, even though it is still a poor fit since the score is extremely negative (0 being the best)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'PCA' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-4da8780d528a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# shown above that only first 4 PCA features have significant variance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpca\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPCA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mscaler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mnormalizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNormalizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mkmeans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKMeans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'PCA' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import SpectralClustering\n",
    "import matplotlib.pyplot as plt\n",
    "from hmmlearn import hmm # Hidden Markovnikov model\n",
    "\n",
    "\n",
    "\n",
    "# shown above that only first 4 PCA features have significant variance\n",
    "pca = PCA(n_components=4)\n",
    "scaler = StandardScaler()\n",
    "normalizer = Normalizer()\n",
    "kmeans = KMeans(n_clusters=3)\n",
    "\n",
    "kpipeline = make_pipeline(pca,kmeans)\n",
    "spipeline = make_pipeline(pca,scaler,kmeans)\n",
    "npipeline = make_pipeline(pca,normalizer,kmeans)\n",
    "\n",
    "kmeans.fit(X_train)\n",
    "kpipeline.fit(X_train)\n",
    "spipeline.fit(X_train)\n",
    "npipeline.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'PCA' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-70a0a976f351>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpca\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPCA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mscaler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnormalizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNormalizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSpectralClustering\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0massign_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'discretize'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'PCA' is not defined"
     ]
    }
   ],
   "source": [
    "pca = PCA(n_components=4)\n",
    "scaler = StandardScaler()\n",
    "normalizer = Normalizer()\n",
    "sc = SpectralClustering(3, assign_labels='discretize')\n",
    "\n",
    "scpipeline = make_pipeline(pca,sc)\n",
    "sscpipeline = make_pipeline(pca,scaler,sc)\n",
    "nscpipeline = make_pipeline(pca,normalizer,sc)\n",
    "\n",
    "scpipeline.fit(X_train)\n",
    "sscpipeline.fit(X_train)\n",
    "nscpipeline.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=4)\n",
    "normalizer = Normalizer()\n",
    "gm = hmm.GaussianHMM(n_components=3)\n",
    "\n",
    "ngmpipeline = make_pipeline(pca,normalizer,gm)\n",
    "\n",
    "ngmpipeline.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The presence of PCA improves raw performance by reducing the number of computations for features with low variance\n",
    "\n",
    "print(\"Outcome Legend (Unsupervised Learning)\")\n",
    "print(\"0: Away win\")\n",
    "print(\"1: Draw\")\n",
    "print(\"2: Home win\")\n",
    "print(\"Note: Supervised learning uses a different outcome legend\\n\")\n",
    "\n",
    "kmeans.fit(X_train, y_train)\n",
    "labels = kmeans.predict(X_test)\n",
    "df1 = pd.DataFrame({'labels': labels, 'wins': y_test})\n",
    "ct = pd.crosstab(df1['labels'],df1['wins'])\n",
    "print('kmeans only')\n",
    "print(ct)\n",
    "print(\"Score:\", kmeans.score(X_test, y_test))\n",
    "print()\n",
    "\n",
    "kpipeline.fit(X_train, y_train)\n",
    "labels = kpipeline.predict(X_test)\n",
    "df1 = pd.DataFrame({'labels': labels, 'wins': y_test})\n",
    "ct = pd.crosstab(df1['labels'],df1['wins'])\n",
    "print('kmeans with PCA')\n",
    "print(ct)\n",
    "print(\"Score:\", kpipeline.score(X_test, y_test))\n",
    "print()\n",
    "\n",
    "spipeline.fit(X_train, y_train)\n",
    "labels = spipeline.predict(X_test)\n",
    "df1 = pd.DataFrame({'labels': labels, 'wins': y_test})\n",
    "ct = pd.crosstab(df1['labels'],df1['wins'])\n",
    "print('kmeans with PCA & scaler')\n",
    "print(ct)\n",
    "print(\"Score:\", spipeline.score(X_test, y_test))\n",
    "print()\n",
    "\n",
    "npipeline.fit(X_train, y_train)\n",
    "labels = npipeline.predict(X_test)\n",
    "df1 = pd.DataFrame({'labels': labels, 'wins': y_test})\n",
    "ct = pd.crosstab(df1['labels'],df1['wins'])\n",
    "print('kmeans with PCA & normalizer')\n",
    "print(ct)\n",
    "print(\"Score:\", npipeline.score(X_test, y_test))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# F1 = 2 * (precision * recall) / (precision + recall)\n",
    "# Using weighted average to account for label imbalance\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "kmeans.fit(X_train, y_train)\n",
    "labels = kmeans.predict(X_test)\n",
    "print('kmeans')\n",
    "print('f1 score: '+ str(f1_score(y_test, labels, average='weighted')))\n",
    "print('accuracy: '+ str(accuracy_score(y_test, labels)))\n",
    "print()\n",
    "\n",
    "kpipeline.fit(X_train, y_train)\n",
    "labels = kpipeline.predict(X_test)\n",
    "print('kmeans with PCA')\n",
    "print('f1 score: '+ str(f1_score(y_test, labels, average='weighted')))\n",
    "print('accuracy: '+ str(accuracy_score(y_test, labels)))\n",
    "print()\n",
    "\n",
    "spipeline.fit(X_train, y_train)\n",
    "labels = spipeline.predict(X_test)\n",
    "print('kmeans with PCA & scaler')\n",
    "print('f1 score: '+ str(f1_score(y_test, labels, average='weighted')))\n",
    "print('accuracy: '+ str(accuracy_score(y_test, labels)))\n",
    "print()\n",
    "\n",
    "npipeline.fit(X_train, y_train)\n",
    "labels = npipeline.predict(X_test)\n",
    "print('kmeans with PCA & normalizer')\n",
    "print('f1 score: '+ str(f1_score(y_test, labels, average='weighted')))\n",
    "print('accuracy: '+ str(accuracy_score(y_test, labels)))\n",
    "print()\n",
    "\n",
    "'''\n",
    "#ngmpipeline.fit(X_train, y_train)\n",
    "labels = ngmpipeline.fit_predict(X_test)\n",
    "print('hmm with PCA & normalizer')\n",
    "print('f1 score: '+ str(f1_score(y_test, labels, average='weighted')))\n",
    "print('accuracy: '+ str(accuracy_score(y_test, labels)))\n",
    "print()\n",
    "'''\n",
    "\n",
    "print('Hence shown that hmm with PCA & normalizer performs the worst')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then proceed to the supervised learning section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.48510886794026864\n",
      "Time taken: 115.58208107948303 seconds\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "rfc = RandomForestClassifier()\n",
    "rfc.fit(X_train, y_train)\n",
    "\n",
    "rfc_pred = rfc.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\", rfc.score(X_test, y_test))\n",
    "\n",
    "t2 = time.time()\n",
    "print(\"Time taken:\", t2-t1, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.4713995717582937\n",
      "Time taken: 108.06315183639526 seconds\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "t1 = time.time()\n",
    "etc = ExtraTreesClassifier()\n",
    "etc.fit(X_train, y_train)\n",
    "\n",
    "etc_pred = etc.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\", etc.score(X_test, y_test))\n",
    "t2 = time.time()\n",
    "print(\"Time taken:\", t2-t1, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING! This may take 10 minutes or so on Kaggle (2020)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of iterations: 10 of 200\n",
      "Time taken: 7.503127098083496 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of iterations: 20 of 200\n",
      "Time taken: 15.905383825302124 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of iterations: 30 of 200\n",
      "Time taken: 26.10327124595642 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of iterations: 40 of 200\n",
      "Time taken: 36.9691641330719 seconds\n",
      "Number of iterations: 50 of 200\n",
      "Time taken: 49.22243666648865 seconds\n",
      "Number of iterations: 60 of 200\n",
      "Time taken: 61.2598717212677 seconds\n",
      "Number of iterations: 70 of 200\n",
      "Time taken: 73.27814245223999 seconds\n",
      "Number of iterations: 80 of 200\n",
      "Time taken: 85.95997095108032 seconds\n",
      "Number of iterations: 90 of 200\n",
      "Time taken: 97.87969994544983 seconds\n",
      "Number of iterations: 100 of 200\n",
      "Time taken: 110.03802442550659 seconds\n",
      "Number of iterations: 110 of 200\n",
      "Time taken: 121.98092722892761 seconds\n",
      "Number of iterations: 120 of 200\n",
      "Time taken: 137.00738334655762 seconds\n",
      "Number of iterations: 130 of 200\n",
      "Time taken: 158.068918466568 seconds\n",
      "Number of iterations: 140 of 200\n",
      "Time taken: 171.03366684913635 seconds\n",
      "Number of iterations: 150 of 200\n",
      "Time taken: 183.05486941337585 seconds\n",
      "Number of iterations: 160 of 200\n",
      "Time taken: 195.0034635066986 seconds\n",
      "Number of iterations: 170 of 200\n",
      "Time taken: 207.69764161109924 seconds\n",
      "Number of iterations: 180 of 200\n",
      "Time taken: 219.63045692443848 seconds\n",
      "Number of iterations: 190 of 200\n",
      "Time taken: 231.7216567993164 seconds\n",
      "Number of iterations: 200 of 200\n",
      "Time taken: 243.898113489151 seconds\n",
      "0.44809654903923696\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "print(\"WARNING! This may take 10 minutes or so on Kaggle (2020)\")\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "for iters in range(10,201,10):\n",
    "    lr = LogisticRegression(max_iter=iters)\n",
    "    lr.fit(X_train, y_train)\n",
    "\n",
    "    lr_pred = lr.predict(X_test)\n",
    "\n",
    "    print(\"Number of iterations:\",  iters, \"of 200\")\n",
    "    #print(lr.score(X_test, y_test))\n",
    "\n",
    "    t2 = time.time()\n",
    "    print(\"Time taken:\", t2-t1, \"seconds\")\n",
    "print(lr.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Random Forest Classifier\")\n",
    "print(rfc.predict_proba(X_test)[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Extra Trees Classifier\")\n",
    "print(etc.predict_proba(X_test)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Logistic Regression\")\n",
    "print(lr.predict_proba(X_test)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = {'RandomForestClassifier' : rfc_pred, 'ExtraTreesClassifier': etc_pred, 'LogisticRegression': lr_pred,\n",
    "        'Actual': y_test}\n",
    "res_df = pd.DataFrame(pred, columns = ['RandomForestClassifier', 'ExtraTreesClassifier', 'LogisticRegression',\n",
    "                                       'Actual'])\n",
    "print(res_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "print(\"Outcome Legend\")\n",
    "print(\"0: Away win\")\n",
    "print(\"1: Draw\")\n",
    "print(\"2: Home win\")\n",
    "print()\n",
    "print(\"Confusion matrix for Random Forest Classifier\")\n",
    "print(confusion_matrix(y_test, res_df['RandomForestClassifier']))\n",
    "print(\"Confusion matrix for Extra Trees Classifier\")\n",
    "print(confusion_matrix(y_test, res_df['ExtraTreesClassifier']))\n",
    "print(\"Confusion matrix for Logistic Regression\")\n",
    "print(confusion_matrix(y_test, res_df['LogisticRegression']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us combine the models and check the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "t1 = time.time()\n",
    "eclf1 = VotingClassifier(estimators=[('1', rfc), ('2', etc), ('3', lr)], voting='hard')\n",
    "eclf1.fit(X_train, y_train)\n",
    "print(\"Accuracy:\", eclf1.score(X_test, y_test))\n",
    "t2 = time.time()\n",
    "print(\"Time taken:\", t2-t1, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "t1 = time.time()\n",
    "eclf2 = VotingClassifier(estimators=[('1', rfc), ('2', etc), ('3', lr)], voting='soft')\n",
    "eclf2.fit(X_train, y_train)\n",
    "print(\"Accuracy:\", eclf2.score(X_test, y_test))\n",
    "t2 = time.time()\n",
    "print(\"Time taken:\", t2-t1, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Confusion matrix for Voting Classifier (hard voting)\")\n",
    "print(confusion_matrix(y_test, eclf1.predict(X_test)))\n",
    "print(\"Confusion matrix for Voting Classifier (soft voting)\")\n",
    "print(confusion_matrix(y_test, eclf2.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also tried the TensorFlow machine learning framework, but it failed terribly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(X_train.shape)\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(X_train.shape[1],)),\n",
    "  tf.keras.layers.Dense(128, activation='tanh'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(3)\n",
    "])\n",
    "\n",
    "X_train_numpy = np.asarray(X_train)\n",
    "y_train_numpy = np.asarray(y_train)\n",
    "X_test_numpy = np.asarray(X_test)\n",
    "y_test_numpy = np.asarray(y_test)\n",
    "\n",
    "predictions = model(np.ndarray.astype(X_train_numpy[:1], np.float32)).numpy()\n",
    "\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "model.compile(optimizer='adam',\n",
    "              loss=loss_fn,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "pre_training_result = model.evaluate(np.ndarray.astype(X_test_numpy, np.float32), \n",
    "                                     np.ndarray.astype(y_test_numpy, np.float32), verbose=2)\n",
    "print(\"Pre-training loss:\", pre_training_result[0])\n",
    "print(\"Pre-training accuracy:\", pre_training_result[1])\n",
    "\n",
    "model.fit(np.ndarray.astype(X_train_numpy, np.float32),\n",
    "          np.ndarray.astype(y_train_numpy, np.float32), epochs=10, validation_split=0.2, batch_size=32)\n",
    "\n",
    "probability_model = tf.keras.Sequential([model, tf.keras.layers.Softmax()])\n",
    "\n",
    "predictions = model(np.ndarray.astype(X_test_numpy, np.float32)).numpy()\n",
    "forecast = np.argmax(predictions, axis=1)\n",
    "\n",
    "post_training_result = model.evaluate(np.ndarray.astype(X_test_numpy, np.float32), \n",
    "                                      np.ndarray.astype(y_test_numpy, np.float32), verbose=2)\n",
    "print(\"Post-training loss:\", post_training_result[0])\n",
    "print(\"Post-training accuracy:\", post_training_result[1])\n",
    "print()\n",
    "\n",
    "print(\"Probability prediction\")\n",
    "print(tf.nn.softmax(predictions).numpy()[:10])\n",
    "print(\"Result prediction\")\n",
    "print(forecast[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Confusion matrix: \")\n",
    "print(confusion_matrix(y_test, forecast))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
